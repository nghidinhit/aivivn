{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 41255,
     "status": "ok",
     "timestamp": 1551920700793,
     "user": {
      "displayName": "Nghị Nguyễn Đình",
      "photoUrl": "",
      "userId": "02222397772048019559"
     },
     "user_tz": -420
    },
    "id": "OdYkLn2hoW8z",
    "outputId": "99aafd4e-b588-4ff5-afd2-74b6d79c8138"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rR8MBgiIfkGV"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/carpedm20/emoji.git\n",
    "# cd emoji\n",
    "# python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 71491,
     "status": "ok",
     "timestamp": 1551920794526,
     "user": {
      "displayName": "Nghị Nguyễn Đình",
      "photoUrl": "",
      "userId": "02222397772048019559"
     },
     "user_tz": -420
    },
    "id": "veq0ty1Wpd_D",
    "outputId": "2a7b4c33-03ec-4b8f-cd3e-60060a48921a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K    100% |████████████████████████████████| 91.1MB 113.1MB/s \n",
      "\u001b[31mfastai 0.7.0 has requirement torch<0.4, but you'll have torch 0.4.1 which is incompatible.\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# http://pytorch.org/\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 43336,
     "status": "ok",
     "timestamp": 1551920794527,
     "user": {
      "displayName": "Nghị Nguyễn Đình",
      "photoUrl": "",
      "userId": "02222397772048019559"
     },
     "user_tz": -420
    },
    "id": "DmxG96IJpxuN",
    "outputId": "17648938-526b-40e4-f0f8-2036ba54d464"
   },
   "outputs": [],
   "source": [
    "# print(accelerator)\n",
    "# path = \"/content/drive/My Drive/AI_COLAB/Colab Notebooks/\"\n",
    "import os\n",
    "import torch\n",
    "path = os.getcwd()\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "# print('cuda: ', USE_CUDA)\n",
    "# device = torch.device('cuda' if USE_CUDA else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UT-Mol6FrEWD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TtSsxQwHqw4P"
   },
   "outputs": [],
   "source": [
    "# path = \"/content/drive/My Drive/AI_COLAB/machinelearningcoban-competition/\"\n",
    "data_path = os.path.join(path, \"SA_demo\")\n",
    "train_file = os.path.join(data_path, \"train.crash\")\n",
    "test_file = os.path.join(data_path, \"test.crash\")\n",
    "train_csv = os.path.join(data_path, \"train.csv\")\n",
    "test_csv = os.path.join(data_path, \"test.csv\")\n",
    "sample_csv = os.path.join(data_path, \"sample.csv\")\n",
    "compare_csv = os.path.join(data_path, \"compare.csv\")\n",
    "diff_csv = os.path.join(data_path, 'diff.csv')\n",
    "diff0_csv = os.path.join(data_path, 'diff0.csv')\n",
    "diff1_csv = os.path.join(data_path, 'diff1.csv')\n",
    "similar_csv = os.path.join(data_path, 'similar.csv')\n",
    "\n",
    "#read file\n",
    "train_pos_file = os.path.join(data_path, \"train_nhan_0.txt\")\n",
    "train_neg_file = os.path.join(data_path, \"train_nhan_1.txt\")\n",
    "\n",
    "test_pos_file = os.path.join(data_path, \"test_nhan_0.txt\")\n",
    "test_neg_file = os.path.join(data_path, \"test_nhan_1.txt\")\n",
    "test_file = os.path.join(data_path, \"test.txt\")\n",
    "\n",
    "\n",
    "#save file\n",
    "train_pos_save = os.path.join(data_path, \"train_pos\")\n",
    "train_neg_save = os.path.join(data_path, \"train_neg\")\n",
    "\n",
    "test_pos_save = os.path.join(data_path, \"test_pos\")\n",
    "test_neg_save = os.path.join(data_path, \"test_neg\")\n",
    "test_save = os.path.join(data_path, \"test\")\n",
    "\n",
    "y_pred_save = os.path.join(data_path, 'y_pred.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yOJQbOF_sWQ3"
   },
   "outputs": [],
   "source": [
    "def load_raw_data(filename, is_train=True):\n",
    "\n",
    "    sample = []\n",
    "    sample_list = []\n",
    "\n",
    "    regex = 'train_'\n",
    "    if not is_train:\n",
    "        regex = 'test_'\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file :\n",
    "            if regex in line:\n",
    "                sample_list.append(sample)\n",
    "                sample = [line]\n",
    "            elif line!='\\n':\n",
    "                sample.append(line)\n",
    "\n",
    "    sample_list.append(sample)      \n",
    "\n",
    "    return sample_list[1:]\n",
    "\n",
    "def save_csv(filename, csvfile, is_train=True):\n",
    "    sample_list = load_raw_data(filename, is_train)\n",
    "\n",
    "    with open(csvfile, 'w') as writer_file:\n",
    "        writer = csv.writer(writer_file)\n",
    "        if is_train:\n",
    "            writer.writerow(['id','comment','label'])\n",
    "        elif not is_train:\n",
    "            writer.writerow(['id', 'comment'])\n",
    "        for sample in sample_list:\n",
    "            id = sample[0].replace('\\n','')\n",
    "            if is_train:\n",
    "                comment = ' SPLIT '.join(sample[1:-1])\n",
    "                comment = comment.replace('\\n','')\n",
    "                label = sample[-1]\n",
    "                writer.writerow([id, comment, label])\n",
    "            else:\n",
    "                comment = ' SPLIT '.join(sample[1:])\n",
    "                comment = comment.replace('\\n','')\n",
    "                writer.writerow([id, comment])\n",
    "# save_csv(train_file, train_csv)\n",
    "# save_csv(test_file, test_csv, is_train=False)\n",
    "  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15469,
     "status": "ok",
     "timestamp": 1551920795654,
     "user": {
      "displayName": "Nghị Nguyễn Đình",
      "photoUrl": "",
      "userId": "02222397772048019559"
     },
     "user_tz": -420
    },
    "id": "BYjBmYrVuq4X",
    "outputId": "c1f64a43-3150-4891-c17a-33fd02af81cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16087 entries, 0 to 16086\n",
      "Data columns (total 3 columns):\n",
      "id         16087 non-null object\n",
      "comment    16087 non-null object\n",
      "label      16087 non-null int64\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 377.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_000000</td>\n",
       "      <td>dùng được sp tốt cảm ơn split shop đóng gói sả...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_000001</td>\n",
       "      <td>chất lượng sản phẩm tuyệt vời sơn mịn nhưng kh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_000002</td>\n",
       "      <td>chất lượng sản phẩm tuyệt vời nhưng không có h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_000003</td>\n",
       "      <td>\":(( Mình hơi thất vọng 1 chút vì mình đã kỳ v...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_000004</td>\n",
       "      <td>lần trước mình mua áo gió màu hồng rất ok mà đ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                            comment  label\n",
       "0  train_000000  dùng được sp tốt cảm ơn split shop đóng gói sả...      0\n",
       "1  train_000001  chất lượng sản phẩm tuyệt vời sơn mịn nhưng kh...      0\n",
       "2  train_000002  chất lượng sản phẩm tuyệt vời nhưng không có h...      0\n",
       "3  train_000003  \":(( Mình hơi thất vọng 1 chút vì mình đã kỳ v...      1\n",
       "4  train_000004  lần trước mình mua áo gió màu hồng rất ok mà đ...      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv = os.path.join(data_path, \"train_correct_addTone.csv\")\n",
    "train_df = pd.read_csv(train_csv)\n",
    "train_df.info()  \n",
    "train_df.head(5)\n",
    "# comments = train_df['comment']\n",
    "# labels = train_df['label']\n",
    "# nhan_0_writer = open(train_pos_file, 'w')\n",
    "# nhan_1_writer = open(train_neg_file, 'w')\n",
    "# for i in range(len(labels)):\n",
    "#     if labels[i] == 0:\n",
    "#         nhan_0_writer.write(comments[i])\n",
    "#     elif labels[i] == 1:\n",
    "#         nhan_1_writer.write(comments[i])\n",
    "#     else:\n",
    "#         print('label: ', labels[i], ' - comment: ', comments[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14328,
     "status": "ok",
     "timestamp": 1551920795991,
     "user": {
      "displayName": "Nghị Nguyễn Đình",
      "photoUrl": "",
      "userId": "02222397772048019559"
     },
     "user_tz": -420
    },
    "id": "SFDkRcotAMDb",
    "outputId": "4c78449f-d98c-4baa-83a2-62e05fd56022"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10981 entries, 0 to 10980\n",
      "Data columns (total 2 columns):\n",
      "id         10981 non-null object\n",
      "comment    10981 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 171.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_000000</td>\n",
       "      <td>chưa dùng thử nên chưa biết\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_000001</td>\n",
       "      <td>không đáng tiềnvì ngay đợt sale nên mới mua nh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_000002</td>\n",
       "      <td>cám ơn shop đóng gói sản phẩm rất đẹp và chắc ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_000003</td>\n",
       "      <td>vải đẹpphom oki luônquá ưng\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_000004</td>\n",
       "      <td>chuẩn hàng đóng gói đẹp\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                            comment\n",
       "0  test_000000                      chưa dùng thử nên chưa biết\\n\n",
       "1  test_000001  không đáng tiềnvì ngay đợt sale nên mới mua nh...\n",
       "2  test_000002  cám ơn shop đóng gói sản phẩm rất đẹp và chắc ...\n",
       "3  test_000003                      vải đẹpphom oki luônquá ưng\\n\n",
       "4  test_000004                          chuẩn hàng đóng gói đẹp\\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_csv = os.path.join(data_path, \"test_correct_addTone.csv\")\n",
    "test_df = pd.read_csv(test_csv)\n",
    "test_df.info()  \n",
    "test_df.head(5)\n",
    "# comments = test_df['comment']\n",
    "# test_writer = open(test_file, 'w')\n",
    "# for comment in comments:\n",
    "#     test_writer.write(comment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCBU9sEL5xyx"
   },
   "outputs": [],
   "source": [
    "bad_word = {\"hơi thất vọng\", \"địt mẹ\", \"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XNuJPg2AD6C8"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.8, max_features=10000, sublinear_tf=True, ngram_range=(1,3))\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_df.comment, train_df.label, test_size=0.3, random_state=42)\n",
    "vectorizer.fit(x_train)\n",
    "x_tfidf_train = vectorizer.transform(x_train)\n",
    "x_tfidf_val = vectorizer.transform(x_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kr-1MWKe6Wav"
   },
   "outputs": [],
   "source": [
    "# print(x_tfidf_train.shape)\n",
    "# print(x_tfidf_val.shape)\n",
    "# print(x_tfidf_train[1].shape)\n",
    "# print('-----------------')\n",
    "# print(x_tfidf_train[1])\n",
    "# print('=================')\n",
    "# print(x_tfidf_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1789,
     "status": "ok",
     "timestamp": 1551773585158,
     "user": {
      "displayName": "Nghị Nguyễn Đình",
      "photoUrl": "",
      "userId": "02222397772048019559"
     },
     "user_tz": -420
    },
    "id": "VHoP125wGlyq",
    "outputId": "8f4941f0-e080-48e2-94c8-e286e44016c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model........\n",
      "Predict...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 89.89%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = LogisticRegression()\n",
    "print(\"Train model........\")\n",
    "sentiment_fit = model.fit(x_tfidf_train, y_train)\n",
    "print(\"Predict...........\")\n",
    "y_pred = sentiment_fit.predict(x_tfidf_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "with open(similar_csv, 'w') as fo:\n",
    "    writer = csv.writer(fo)\n",
    "    writer.writerow(['index', 'comment', 'label', 'predict'])\n",
    "    df = pd.concat([x_val, y_val], axis=1)\n",
    "    comment = x_val.values\n",
    "    index = x_val.index\n",
    "    label = y_val.values\n",
    "    for i in range(len(comment)):\n",
    "        if label[i] == y_pred[i]:\n",
    "            writer.writerow([index[i], comment[i], label[i], y_pred[i]])\n",
    "print(\"accuracy score: {0:.2f}%\".format(accuracy*100))\n",
    "\n",
    "x_tfidf_test = vectorizer.transform(test_df.comment)\n",
    "y_predict = sentiment_fit.predict(x_tfidf_test)\n",
    "test_df['label'] = y_predict\n",
    "test_df[['id', 'label']].to_csv(sample_csv, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26892,
     "status": "ok",
     "timestamp": 1551671370630,
     "user": {
      "displayName": "Nghị Nguyễn Đình",
      "photoUrl": "",
      "userId": "02222397772048019559"
     },
     "user_tz": -420
    },
    "id": "cneiBMDVjGz4",
    "outputId": "c9a9d111-a803-4843-9831-72c325628c1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict...........\n",
      "accuracy score: 89.66%\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "model = svm.SVC(kernel='linear')\n",
    "sentiment_fit = model.fit(x_tfidf_train, y_train)\n",
    "print(\"Predict...........\")\n",
    "y_pred = sentiment_fit.predict(x_tfidf_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"accuracy score: {0:.2f}%\".format(accuracy*100))\n",
    "\n",
    "x_tfidf_test = vectorizer.transform(test_df.comment)\n",
    "y_predict = sentiment_fit.predict(x_tfidf_test)\n",
    "test_df['label'] = y_predict\n",
    "sample_csv = os.path.join(data_path, \"sample_svm.csv\")\n",
    "test_df[['id', 'label']].to_csv(sample_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QzCbWNCgFp4n"
   },
   "outputs": [],
   "source": [
    "# import emoji\n",
    "\n",
    "# def extract_emojis(str):\n",
    "#     return [c for c in str if c in emoji.UNICODE_EMOJI]\n",
    "\n",
    "# good_df = train_df[train_df['label'] == 0]\n",
    "# good_cmt = good_df['comment'].values\n",
    "# good_emoji = []\n",
    "# for cmt in good_gmt:\n",
    "#     emoji = extract_emoji(cmt)\n",
    "#     good_emoji.extend(emoji)\n",
    "    \n",
    "# good_emoji = np.unique(np.asarray(good_emoji))\n",
    "\n",
    "\n",
    "# bad_df = train_df[train_df['label'] == 1]\n",
    "# bad_cmt = bad_df['comment'].values\n",
    "# bad_emoj = []\n",
    "# for cmt in bad_cmt:\n",
    "#     emoji = extract_emoji(cmt)\n",
    "#     bad_emoji.extend(emoji)\n",
    "\n",
    "# bad_emoji = np.unique(np.asrray(bad_emoji))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o0PdwRVBnJh6"
   },
   "outputs": [],
   "source": [
    "# path = \"/content/drive/My Drive/AI_COLAB/machinelearningcoban-competition/SA_demo/pystacknet-master/setup.py\"\n",
    "# print(path)\n",
    "# !python \"/content/drive/My Drive/AI_COLAB/machinelearningcoban-competition/SA_demo/pystacknet-master/setup.py\" install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GPeTK4CokQge"
   },
   "outputs": [],
   "source": [
    "from pystacknet.pystacknet import StackNetClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "models=[ \n",
    "    ######## First level ########\n",
    "    [\n",
    "        RandomForestClassifier (n_estimators=100, criterion=\"entropy\", max_depth=5, max_features=0.5, random_state=1),\n",
    "        ExtraTreesClassifier (n_estimators=100, criterion=\"entropy\", max_depth=5, max_features=0.5, random_state=1),\n",
    "        GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, max_features=0.5, random_state=1),\n",
    "        LogisticRegression(random_state=1)\n",
    "    ],\n",
    "    ######## Second level ########\n",
    "    [\n",
    "        RandomForestClassifier (n_estimators=200, criterion=\"entropy\", max_depth=5, max_features=0.5, random_state=1)\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "model = StackNetClassifier(\n",
    "    models, metric=\"f1\", \n",
    "    folds=5,\n",
    "    restacking=False, \n",
    "    use_retraining=True, \n",
    "    use_proba=True, \n",
    "    random_state=12345, n_jobs=1, verbose=1\n",
    ")\n",
    "\n",
    "model.fit(x_tfidf_train, y_train)\n",
    "# preds=model.predict_proba(x_tfidf_val)\n",
    "# pred_cls = np.argmax(preds, axis=1)\n",
    "# submission = pd.read_csv(\"./data/SA_demo/sample_submission.csv\")\n",
    "# submission['label'] = pred_cls\n",
    "# submission.to_csv(\"stack_demo.csv\", index=False)\n",
    "\n",
    "x_tfidf_test = vectorizer.transform(test_df.comment)\n",
    "y_predict = model.predict(x_tfidf_test)\n",
    "test_df['label'] = y_predict\n",
    "sample_csv = os.path.join(data_path, \"sample_svm.csv\")\n",
    "test_df[['id', 'label']].to_csv(sample_csv, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gP7dsDEFtaag"
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_predict\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# models = [\n",
    "#     RandomForestClassifier (n_estimators=100, criterion=\"entropy\", max_depth=5, max_features=0.5, random_state=1),\n",
    "#     ExtraTreesClassifier (n_estimators=100, criterion=\"entropy\", max_depth=5, max_features=0.5, random_state=1),\n",
    "#     GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, max_features=0.5, random_state=1),\n",
    "#     LogisticRegression(random_state=1)\n",
    "# ]\n",
    "\n",
    "# def cross_val_and_predict(clf, X, y, X_test, nfolds):\n",
    "#     kf = StratifiedKFold(n_splits=nfolds, shuffle=True, random_state=42)\n",
    "    \n",
    "#     oof_preds = np.zeros((X.shape[0], 2))\n",
    "#     sub_preds = np.zeros((X_test.shape[0], 2))\n",
    "    \n",
    "#     for fold, (train_idx, valid_idx) in enumerate(kf.split(X, y)):\n",
    "#         X_train, y_train = X[train_idx], y[train_idx]\n",
    "#         X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "        \n",
    "#         print('===========================')\n",
    "#         print(len(np.argwhere(np.isnan(y_train))))\n",
    "#         print(np.any(np.isnan(y_train)))\n",
    "#         print(np.all(np.isfinite(y_train)))\n",
    "#         mask = np.all(np.isnan(y_train))\n",
    "#         y_train[~mask]\n",
    "#         print('===========================')\n",
    "        \n",
    "#         clf.fit(X_train, y_train)\n",
    "        \n",
    "#         oof_preds[valid_idx] = clf.predict_proba(X_valid)\n",
    "#         sub_preds += clf.predict_proba(X_test) / kf.n_splits\n",
    "        \n",
    "#     return oof_preds, sub_preds\n",
    "\n",
    "# sub_preds = []\n",
    "\n",
    "# for clf in models:\n",
    "#     oof_pred, sub_pred = cross_val_and_predict(clf, x_tfidf_train, y_train, x_tfidf_test, nfolds=5)\n",
    "#     oof_pred_cls = oof_pred.argmax(axis=1)\n",
    "#     oof_f1 = f1_score(y_pred=oof_pred_cls, y_true=y_train)\n",
    "    \n",
    "#     print(clf.__class__)\n",
    "#     print(f\"F1 CV: {oof_f1}\")\n",
    "    \n",
    "#     sub_preds.append(sub_pred)\n",
    "\n",
    "# sub_preds = np.asarray(sub_preds)\n",
    "# sub_preds = sub_preds.mean(axis=0)\n",
    "# sub_pred_cls = sub_preds.argmax(axis=1)\n",
    "\n",
    "# submission_ensemble = submission.copy()\n",
    "# submission_ensemble['label'] = sub_pred_cls\n",
    "# submission_ensemble.to_csv(\"ensemble.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6755,
     "status": "error",
     "timestamp": 1551836873968,
     "user": {
      "displayName": "Nghị Nguyễn Đình",
      "photoUrl": "",
      "userId": "02222397772048019559"
     },
     "user_tz": -420
    },
    "id": "uAqO6X0FyPZu",
    "outputId": "4631baf1-58e1-40ea-f0ac-7383809fe7ac"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-6d7c8c067277>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mtrain_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtest_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "## Plotly\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "# Others\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "### Create sequence\n",
    "vocabulary_size = 10000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(train_df['comment'])\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df['comment'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df['comment'])\n",
    "x_train = pad_sequences(train_sequences, maxlen=50)\n",
    "x_test = pad_sequences(test_sequences, maxlen=50)\n",
    "labels = train_df['label']\n",
    "\n",
    "\n",
    "def create_conv_model():\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(Embedding(vocabulary_size, 100, input_length=50))\n",
    "#     model_conv.add(Dropout(0.2))\n",
    "#     model_conv.add(Conv1D(64, 5, activation='relu'))\n",
    "#     model_conv.add(MaxPooling1D(pool_size=4))\n",
    "    model_conv.add(LSTM(100))\n",
    "    model_conv.add(Dense(1, activation='sigmoid'))\n",
    "    model_conv.compile(loss='binary_crossentropy', optimizer='adam',    metrics=['accuracy'])\n",
    "    return model_conv\n",
    "# model_conv = create_conv_model()\n",
    "# model_conv.fit(x_train, np.array(labels), validation_split=0.4, epochs = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7132,
     "status": "ok",
     "timestamp": 1551694416700,
     "user": {
      "displayName": "Nghị Nguyễn Đình",
      "photoUrl": "",
      "userId": "02222397772048019559"
     },
     "user_tz": -420
    },
    "id": "g9Q7D0EO1gFa",
    "outputId": "e0bf658e-48ff-40b9-c7ef-9cf11332117c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      "Loaded model from disk\n",
      "sample:  /content/drive/My Drive/AI_COLAB/machinelearningcoban-competition/SA_demo/sample_lstm.csv\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "# scores = model.evaluate(X, Y, verbose=0)\n",
    "# print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    " \n",
    "# serialize model to JSON\n",
    "model_json = model_conv.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model_conv.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " \n",
    "# later...\n",
    " \n",
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "\n",
    "y_predict = model_conv.predict_classes(x_test)\n",
    "test_df['label'] = y_predict\n",
    "sample_csv = os.path.join(data_path, \"sample_lstm.csv\")\n",
    "print('sample: ', sample_csv)\n",
    "test_df[['id', 'label']].to_csv(sample_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46624,
     "status": "ok",
     "timestamp": 1551691847809,
     "user": {
      "displayName": "Nghị Nguyễn Đình",
      "photoUrl": "",
      "userId": "02222397772048019559"
     },
     "user_tz": -420
    },
    "id": "hBDhM_7PBNKh",
    "outputId": "c8c47b4f-d341-420e-ef78-661cfb978435"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9652 samples, validate on 6435 samples\n",
      "Epoch 1/3\n",
      "9652/9652 [==============================] - 12s 1ms/step - loss: 0.5443 - acc: 0.7271 - val_loss: 0.4902 - val_acc: 0.7605\n",
      "Epoch 2/3\n",
      "9652/9652 [==============================] - 10s 1ms/step - loss: 0.4829 - acc: 0.7712 - val_loss: 0.4802 - val_acc: 0.7667\n",
      "Epoch 3/3\n",
      "9652/9652 [==============================] - 10s 1ms/step - loss: 0.4587 - acc: 0.7857 - val_loss: 0.4640 - val_acc: 0.7779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7cf4a9ef98>"
      ]
     },
     "execution_count": 116,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "with open(os.path.join(data_path, 'glove.6B.100d.txt')) as fo:\n",
    "    for line in fo:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    fo.close()\n",
    "         \n",
    "         \n",
    "embedding_matrix = np.zeros((vocabulary_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > vocabulary_size - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "         \n",
    "## create model\n",
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(vocabulary_size, 100, input_length=50, weights=[embedding_matrix], trainable=False))\n",
    "model_glove.add(Dropout(0.2))\n",
    "model_glove.add(Conv1D(64, 5, activation='relu'))\n",
    "model_glove.add(MaxPooling1D(pool_size=4))\n",
    "model_glove.add(LSTM(100))\n",
    "model_glove.add(Dense(1, activation='sigmoid'))\n",
    "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "## Fit train data\n",
    "model_glove.fit(x_train, np.array(labels), validation_split=0.4, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5589,
     "status": "ok",
     "timestamp": 1551920811040,
     "user": {
      "displayName": "Nghị Nguyễn Đình",
      "photoUrl": "",
      "userId": "02222397772048019559"
     },
     "user_tz": -420
    },
    "id": "9jFpYsylZbva",
    "outputId": "8e7f4d02-5605-492b-d76c-01fdfaf41a95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('len pos: ', 9280)\n",
      "('avg len: ', 18)\n",
      "('len pos: ', 6807)\n",
      "('avg len: ', 49)\n",
      "('len pos: ', 10981)\n",
      "('avg len: ', 51)\n",
      "('word2id: ', 15706)\n",
      "('maxlen', 639)\n",
      "('maxlen250', 30)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import numpy\n",
    "\n",
    "\t\n",
    "word2id= {}\n",
    "id2word={}\n",
    "index = 1\n",
    "maxlen = 0\n",
    "avglen = 0\n",
    "count100 = 0\n",
    "\n",
    "open_files = [train_pos_file, train_neg_file, test_file]\n",
    "\n",
    "save_files = [train_pos_save, train_neg_save, test_save]\n",
    "\n",
    "for open_file, save_file in zip(open_files,save_files):\n",
    "    pos = []\n",
    "    file = open(open_file, 'r')\n",
    "\n",
    "    for aline in file.readlines():\n",
    "        aline = aline.replace('\\n', \"\")\n",
    "        ids = np.array([], dtype='int32')\n",
    "        for word in aline.split(' '):\n",
    "            word = word.lower()\n",
    "            if word in word2id:\n",
    "                ids = np.append(ids, word2id[word])\n",
    "            else:\n",
    "                if word != '':\n",
    "                    # print (word, \"not in vocalbulary\")\n",
    "                    word2id[word] = index\n",
    "                    id2word[index] = word\n",
    "                    ids = np.append(ids, index)\n",
    "                    index = index + 1\n",
    "        if len(ids) > 0:\n",
    "            pos.append(ids)\n",
    "\n",
    "    file.close()\n",
    "    print(('len pos: ', len(pos)))\n",
    "    np.save(save_file, pos)\n",
    "    for li in pos:\n",
    "        if maxlen < len(li):\n",
    "            maxlen = len(li)\n",
    "        avglen += len(li)\n",
    "        if len(li) > 250:\n",
    "            count100+=1\n",
    "    print('avg len: ', avglen / len(pos))\n",
    "\n",
    "print(('word2id: ', len(word2id)))\n",
    "print((\"maxlen\",maxlen))\n",
    "print((\"maxlen250\",count100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lynBzBOpoBW_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "# import cPickle\n",
    "\n",
    "def load_data_shuffle():\n",
    "    train_pos_save = os.path.join(data_path, \"train_pos.npy\")\n",
    "    train_neg_save = os.path.join(data_path, \"train_neg.npy\")\n",
    "    #Load train data\n",
    "    pos_train = np.load(train_pos_save,encoding='ASCII')\n",
    "    neg_train = np.load(train_neg_save,encoding='ASCII')\n",
    "\n",
    "    y_pos_train = []\n",
    "    for i in pos_train:\n",
    "        y_pos_train.append([0,1])\n",
    "    y_pos_train = np.array(y_pos_train)\n",
    "\n",
    "    y_neg_train = []\n",
    "    for i in neg_train:\n",
    "        y_neg_train.append([1, 0])\n",
    "    y_neg_train = np.array(y_neg_train)\n",
    "    \n",
    "#     print('pos train shape: ', pos_train.shape)\n",
    "    print(pos_train)\n",
    "    #split train and validate set\n",
    "    val_len = int(round(len(pos_train)/10))\n",
    "#     print('val_len: ', val_len)\n",
    "\n",
    "    pos_val = pos_train[0:val_len]\n",
    "    pos_train = pos_train[val_len:]\n",
    "    y_pos_val = y_pos_train[0:val_len]\n",
    "    y_pos_train = y_pos_train[val_len:]\n",
    "\n",
    "\n",
    "    neg_val = neg_train[0:val_len]\n",
    "    neg_train = neg_train[val_len:]\n",
    "    y_neg_val = y_neg_train[0:val_len]\n",
    "    y_neg_train = y_neg_train[val_len:]\n",
    "\n",
    "\n",
    "    X_train = np.concatenate([pos_train,neg_train])\n",
    "    y_train = np.concatenate([y_pos_train,y_neg_train])\n",
    "\n",
    "    X_val = np.concatenate([pos_val,neg_val])\n",
    "    y_val = np.concatenate([y_pos_val,y_neg_val])\n",
    "\n",
    "    test_save = os.path.join(data_path, \"test.npy\")\n",
    "    X_test = np.load(test_save, encoding='ASCII')\n",
    "#     X_test = np.concatenate([pos_test,neu_test,neg_test])\n",
    "#     y_test = np.concatenate([y_pos_test,y_neu_test,y_neg_test])\n",
    "\n",
    "    # print X_train.shape, y_train.shape\n",
    "    # print X_test.shape, y_test.shape\n",
    "    # print X_val.shape, y_val.shape\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1498
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1331488,
     "status": "error",
     "timestamp": 1551856982589,
     "user": {
      "displayName": "Nghị Nguyễn Đình",
      "photoUrl": "",
      "userId": "02222397772048019559"
     },
     "user_tz": -420
    },
    "id": "9mL3D4B71u5T",
    "outputId": "cd8becdc-e422-4816-a21d-8c2e6f79debb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "[array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "       18, 19, 11, 12, 20, 21])\n",
      " array([18, 19, 11, 12, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32])\n",
      " array([18, 19, 11, 12, 20, 21, 24, 28, 33, 34, 28, 33, 35, 36, 37, 28, 33,\n",
      "       38])\n",
      " ... array([ 53,  14, 145,  78, 291])\n",
      " array([301, 302,  67,  68,  69,  70,   9,  10,  11,  12,  13,  14,  15,\n",
      "        16,  17, 167, 168,  81,  53,  13,  54,   8,  64,  65,  13,   4])\n",
      " array([172, 173, 175,  14])]\n",
      "14231 train sequences\n",
      "10981 test sequences\n",
      "X_train shape: (14231, 50)\n",
      "X_val shape: (1856, 50)\n",
      "X_test shape: (10981, 50)\n",
      "Build model...\n",
      "(?, 150)\n",
      "(?, 150)\n",
      "(?, 150)\n",
      "(?, 150)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:85: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", padding=\"valid\", strides=1, filters=150, kernel_size=3)`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:97: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", padding=\"valid\", strides=1, filters=150, kernel_size=5)`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:109: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", padding=\"valid\", strides=1, filters=150, kernel_size=7)`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:138: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=[<tf.Tenso...)`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:151: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14231 samples, validate on 1856 samples\n",
      "Epoch 1/20\n",
      "14231/14231 [==============================] - 289s 20ms/step - loss: 0.2909 - acc: 0.8756 - val_loss: 0.2346 - val_acc: 0.9084\n",
      "Epoch 2/20\n",
      "14231/14231 [==============================] - 298s 21ms/step - loss: 0.2008 - acc: 0.9252 - val_loss: 0.2336 - val_acc: 0.9095\n",
      "Epoch 3/20\n",
      "14231/14231 [==============================] - 301s 21ms/step - loss: 0.1392 - acc: 0.9509 - val_loss: 0.2715 - val_acc: 0.8928\n",
      "Epoch 4/20\n",
      " 2810/14231 [====>.........................] - ETA: 3:51 - loss: 0.0760 - acc: 0.9762Loading data ...\n",
      "[array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "       18, 19, 11, 12, 20, 21])\n",
      " array([18, 19, 11, 12, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32])\n",
      " array([18, 19, 11, 12, 20, 21, 24, 28, 33, 34, 28, 33, 35, 36, 37, 28, 33,\n",
      "       38])\n",
      " ... array([ 53,  14, 145,  78, 291])\n",
      " array([301, 302,  67,  68,  69,  70,   9,  10,  11,  12,  13,  14,  15,\n",
      "        16,  17, 167, 168,  81,  53,  13,  54,   8,  64,  65,  13,   4])\n",
      " array([172, 173, 175,  14])]\n",
      "14231 train sequences\n",
      "10981 test sequences\n",
      "X_train shape: (14231, 50)\n",
      "X_val shape: (1856, 50)\n",
      "X_test shape: (10981, 50)\n",
      "Build model...\n",
      "(?, 150)\n",
      "(?, 150)\n",
      "(?, 150)\n",
      "(?, 150)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:85: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", padding=\"valid\", strides=1, filters=150, kernel_size=3)`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:97: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", padding=\"valid\", strides=1, filters=150, kernel_size=5)`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:109: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", padding=\"valid\", strides=1, filters=150, kernel_size=7)`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:138: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=[<tf.Tenso...)`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:151: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14231 samples, validate on 1856 samples\n",
      "Epoch 1/20\n",
      "14231/14231 [==============================] - 289s 20ms/step - loss: 0.2909 - acc: 0.8756 - val_loss: 0.2346 - val_acc: 0.9084\n",
      "Epoch 2/20\n",
      "14231/14231 [==============================] - 298s 21ms/step - loss: 0.2008 - acc: 0.9252 - val_loss: 0.2336 - val_acc: 0.9095\n",
      "Epoch 3/20\n",
      "14231/14231 [==============================] - 301s 21ms/step - loss: 0.1392 - acc: 0.9509 - val_loss: 0.2715 - val_acc: 0.8928\n",
      "Epoch 4/20\n",
      "14231/14231 [==============================] - 298s 21ms/step - loss: 0.0753 - acc: 0.9773 - val_loss: 0.3337 - val_acc: 0.8809\n",
      "14231/14231 [==============================] - 298s 21ms/step - loss: 0.0753 - acc: 0.9773 - val_loss: 0.3337 - val_acc: 0.8809\n",
      "Epoch 5/20\n",
      "Epoch 5/20\n",
      "14231/14231 [==============================] - 298s 21ms/step - loss: 0.0375 - acc: 0.9888 - val_loss: 0.3878 - val_acc: 0.8750\n",
      "14231/14231 [==============================] - 298s 21ms/step - loss: 0.0375 - acc: 0.9888 - val_loss: 0.3878 - val_acc: 0.8750\n",
      "Epoch 6/20\n",
      "Epoch 6/20\n",
      "14231/14231 [==============================] - 308s 22ms/step - loss: 0.0212 - acc: 0.9940 - val_loss: 0.3952 - val_acc: 0.8928\n",
      "14231/14231 [==============================] - 308s 22ms/step - loss: 0.0212 - acc: 0.9940 - val_loss: 0.3952 - val_acc: 0.8928\n",
      "Epoch 7/20\n",
      "Epoch 7/20\n",
      "14231/14231 [==============================] - 299s 21ms/step - loss: 0.0128 - acc: 0.9963 - val_loss: 0.4723 - val_acc: 0.8858\n",
      "14231/14231 [==============================] - 299s 21ms/step - loss: 0.0128 - acc: 0.9963 - val_loss: 0.4723 - val_acc: 0.8858\n",
      "Epoch 8/20\n",
      "Epoch 8/20\n",
      "13160/14231 [==========================>...] - ETA: 21s - loss: 0.0085 - acc: 0.9977"
     ]
    }
   ],
   "source": [
    "'''This example demonstrates the use of Convolution1D for text classification.\n",
    "Gets to 0.88 test accuracy after 2 epochs.\n",
    "90s/epoch on Intel i5 2.4Ghz CPU.\n",
    "10s/epoch on Tesla K40 GPU.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D, LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.preprocessing import sequence as sq\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda,merge,Input,TimeDistributed,Flatten, Add\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import keras.backend.tensorflow_backend as K\n",
    "\n",
    "#config = K.tf.ConfigProto(intra_op_parallelism_threads=16, inter_op_parallelism_threads=16, \\\n",
    "#                        allow_soft_placement=True, device_count = {'CPU': 1})\n",
    "\n",
    "\n",
    "# tf_config = K.tf.ConfigProto()\n",
    "# tf_config.gpu_options.allow_growth = True\n",
    "# session = K.tf.Session(config=tf_config)\n",
    "# K.set_session(session)\n",
    "\n",
    "# config = K.tf.ConfigProto(intra_op_parallelism_threads=4, inter_op_parallelism_threads=4, \\\n",
    "#                         allow_soft_placement=True, device_count = {'CPU': 4})\n",
    "# session = K.tf.Session(config=config)\n",
    "# K.set_session(session)\n",
    "\n",
    "# set parameters:\n",
    "max_features = 20000#14300\n",
    "maxlen = 50\n",
    "batch_size = 10\n",
    "embedding_dims = 200\n",
    "nb_filter = 150\n",
    "filter_length = 3\n",
    "hidden_dims = 100\n",
    "nb_epoch = 20\n",
    "\n",
    "accs = []\n",
    "print('Loading data ...', )\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test = load_data_shuffle()\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "X_train = sq.pad_sequences(X_train, maxlen=maxlen, padding='pre', truncating='post', value=0.0)\n",
    "X_test = sq.pad_sequences(X_test, maxlen=maxlen, padding='pre', truncating='post', value=0.0)\n",
    "X_val = sq.pad_sequences(X_val, maxlen=maxlen, padding='pre', truncating='post', value=0.0)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_val shape:', X_val.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "input_layer = Input(shape=(maxlen,),dtype='int32', name='main_input')\n",
    "emb_layer = Embedding(max_features,\n",
    "                      embedding_dims,\n",
    "                      input_length=maxlen\n",
    "                      )(input_layer)\n",
    "def max_1d(X):\n",
    "    return K.max(X, axis=1)\n",
    "\n",
    "# we add a Convolution1D, which will learn nb_filter\n",
    "# word group filters of size 3:\n",
    "\n",
    "con3_layer = Convolution1D(nb_filter=nb_filter,\n",
    "                    filter_length=3,\n",
    "                    border_mode='valid',\n",
    "                    activation='relu',\n",
    "                    subsample_length=1)(emb_layer)\n",
    "\n",
    "pool_con3_layer = Lambda(max_1d, output_shape=(nb_filter,))(con3_layer)\n",
    "\n",
    "\n",
    "# we add a Convolution1D, which will learn nb_filter\n",
    "# word group filters of size 4:\n",
    "\n",
    "con4_layer = Convolution1D(nb_filter=nb_filter,\n",
    "                    filter_length=5,\n",
    "                    border_mode='valid',\n",
    "                    activation='relu',\n",
    "                    subsample_length=1)(emb_layer)\n",
    "\n",
    "pool_con4_layer = Lambda(max_1d, output_shape=(nb_filter,))(con4_layer)\n",
    "\n",
    "\n",
    "# we add a Convolution1D, which will learn nb_filter\n",
    "# word group filters of size 5:\n",
    "\n",
    "con5_layer = Convolution1D(nb_filter=nb_filter,\n",
    "                    filter_length=7,\n",
    "                    border_mode='valid',\n",
    "                    activation='relu',\n",
    "                    subsample_length=1)(emb_layer)\n",
    "\n",
    "pool_con5_layer = Lambda(max_1d, output_shape=(nb_filter,))(con5_layer)\n",
    "\n",
    "cnn_layer = Add()([pool_con3_layer, pool_con5_layer,pool_con4_layer])\n",
    "print(pool_con3_layer.shape)\n",
    "print(pool_con5_layer.shape)\n",
    "print(pool_con4_layer.shape)\n",
    "print(cnn_layer.shape)\n",
    "\n",
    "# cnn_layer = merge([pool_con3_layer, pool_con5_layer,pool_con4_layer ], mode='concat')\n",
    "\n",
    "\n",
    "#LSTM\n",
    "\n",
    "\n",
    "x = Embedding(max_features, embedding_dims, input_length=maxlen)(input_layer)\n",
    "lstm_layer = LSTM(150)(x)\n",
    "\n",
    "cnn_lstm_layer = Add()([lstm_layer, cnn_layer])\n",
    "# cnn_lstm_layer = merge([lstm_layer, cnn_layer], mode='concat')\n",
    "\n",
    "dense_layer = Dense(hidden_dims*2, activation='sigmoid')(cnn_lstm_layer)\n",
    "output_layer= Dropout(0.2)(dense_layer)\n",
    "output_layer = Dense(2, trainable=True, activation='sigmoid')(output_layer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Model(input=[input_layer], output=[output_layer])\n",
    "adadelta = Adadelta(lr=0.5, rho=0.95, epsilon=1e-06)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=\"adamax\",\n",
    "              metrics=['accuracy'])\n",
    "checkpoint = ModelCheckpoint(os.path.join(data_path, 'weights.hdf5'),\n",
    "                             monitor='val_acc', verbose=0, save_best_only=True,\n",
    "                             mode='max')\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=nb_epoch,\n",
    "          callbacks=[checkpoint],\n",
    "          validation_data=(X_val, y_val))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 32385,
     "status": "ok",
     "timestamp": 1551940339186,
     "user": {
      "displayName": "Nghị Nguyễn Đình",
      "photoUrl": "",
      "userId": "02222397772048019559"
     },
     "user_tz": -420
    },
    "id": "ZnMc8JjC53A1",
    "outputId": "6143508d-acc4-4bf7-f4ee-5d510983278b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1856/1856 [==============================] - 5s 3ms/step\n",
      "shape:  (10981, 2)\n",
      "[0.01201731 0.7761873 ]\n",
      "type:  <type 'numpy.ndarray'>\n",
      "shape:  (2,)\n",
      "0.01201731\n",
      "0.7761873\n",
      "[[0.01201731 0.7761873 ]\n",
      " [0.9168888  0.02287787]\n",
      " [0.00254977 0.6909789 ]\n",
      " ...\n",
      " [0.03018633 0.61407846]\n",
      " [0.00185339 0.80657166]\n",
      " [0.04191924 0.4418036 ]]\n",
      "Acc: 0.9084051660551079\n",
      "Score:  0.23495351318079288\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(os.path.join(data_path, 'weights.hdf5'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=\"adamax\",\n",
    "              metrics=['accuracy'])\n",
    "score, acc = model.evaluate(X_val, y_val, batch_size=batch_size)\n",
    "y_predict = model.predict(X_test, batch_size=batch_size)\n",
    "cls = []\n",
    "print('shape: ', y_predict.shape)\n",
    "print(y_predict[0])\n",
    "print('type: ', type(y_predict[0]))\n",
    "print('shape: ', y_predict[0].shape)\n",
    "for x in y_predict[0]:\n",
    "    print(x)\n",
    "for i in range(len(y_predict)):\n",
    "    if y_predict[i][0] < y_predict[i][1]: \n",
    "        cls.append(0)\n",
    "    else: cls.append(1)\n",
    "print(y_predict)\n",
    "test_df['label'] = np.asarray(cls)\n",
    "sample_csv = os.path.join(data_path, \"sample_cnn-lstm.csv\")\n",
    "test_df[['id', 'label']].to_csv(sample_csv, index=False)\n",
    "\n",
    "print(\"Acc:\", acc)\n",
    "print(\"Score: \", score)\n",
    "# accs.append(acc)\n",
    "# print (\"Average Acc:\", K.np.mean(K.np.array(accs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ONSXYFMj_tyI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "contest1_sentiment-classification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
