{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"contest1_sentiment-classification.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python2","display_name":"Python 2"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"OdYkLn2hoW8z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"outputId":"1e92abf8-0fd8-408f-f48f-cc86fd0de7f5","executionInfo":{"status":"ok","timestamp":1552271067190,"user_tz":-420,"elapsed":27036,"user":{"displayName":"Nghị Nguyễn Đình","photoUrl":"","userId":"02222397772048019559"}}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"metadata":{"id":"rR8MBgiIfkGV","colab_type":"code","colab":{}},"cell_type":"code","source":["# !git clone https://github.com/carpedm20/emoji.git\n","# cd emoji\n","# python setup.py install"],"execution_count":0,"outputs":[]},{"metadata":{"id":"veq0ty1Wpd_D","colab_type":"code","outputId":"159b98f8-2cf8-46b5-82ad-38306f2d5151","executionInfo":{"status":"ok","timestamp":1552274128445,"user_tz":-420,"elapsed":7141,"user":{"displayName":"Nghị Nguyễn Đình","photoUrl":"","userId":"02222397772048019559"}},"colab":{"base_uri":"https://localhost:8080/","height":90}},"cell_type":"code","source":["# http://pytorch.org/\n","from os.path import exists\n","from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n","accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n","!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n","import torch"],"execution_count":22,"outputs":[{"output_type":"stream","text":["\u001b[31m  HTTP error 403 while getting http://download.pytorch.org/whl/cu100/torch-0.4.1-cp27-cp27mu-linux_x86_64.whl\u001b[0m\n","\u001b[31m  Could not install requirement torch==0.4.1 from http://download.pytorch.org/whl/cu100/torch-0.4.1-cp27-cp27mu-linux_x86_64.whl because of error 403 Client Error: Forbidden for url: http://download.pytorch.org/whl/cu100/torch-0.4.1-cp27-cp27mu-linux_x86_64.whl\u001b[0m\n","\u001b[31mCould not install requirement torch==0.4.1 from http://download.pytorch.org/whl/cu100/torch-0.4.1-cp27-cp27mu-linux_x86_64.whl because of HTTP error 403 Client Error: Forbidden for url: http://download.pytorch.org/whl/cu100/torch-0.4.1-cp27-cp27mu-linux_x86_64.whl for URL http://download.pytorch.org/whl/cu100/torch-0.4.1-cp27-cp27mu-linux_x86_64.whl\u001b[0m\n"],"name":"stdout"}]},{"metadata":{"id":"DmxG96IJpxuN","colab_type":"code","outputId":"749d03ba-3131-4cd0-9412-bfa5f540622b","executionInfo":{"status":"ok","timestamp":1552274154753,"user_tz":-420,"elapsed":968,"user":{"displayName":"Nghị Nguyễn Đình","photoUrl":"","userId":"02222397772048019559"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["print(accelerator)\n","path = \"/content/drive/My Drive/AI_COLAB/Colab Notebooks/\"\n","USE_CUDA = torch.cuda.is_available()\n","print('cuda: ', USE_CUDA)\n","device = torch.device('cuda' if USE_CUDA else 'cpu')"],"execution_count":25,"outputs":[{"output_type":"stream","text":["cu100\n","('cuda: ', True)\n"],"name":"stdout"}]},{"metadata":{"id":"UT-Mol6FrEWD","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","import pandas as pd\n","import csv\n","import numpy as np"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TtSsxQwHqw4P","colab_type":"code","colab":{}},"cell_type":"code","source":["path = \"/content/drive/My Drive/AI_COLAB/machinelearningcoban-competition/\"\n","data_path = os.path.join(path, \"SA_demo\")\n","train_file = os.path.join(data_path, \"train.crash\")\n","test_file = os.path.join(data_path, \"test.crash\")\n","train_csv = os.path.join(data_path, \"train.csv\")\n","test_csv = os.path.join(data_path, \"test.csv\")\n","sample_csv = os.path.join(data_path, \"sample.csv\")\n","compare_csv = os.path.join(data_path, \"compare.csv\")\n","diff_csv = os.path.join(data_path, 'diff.csv')\n","diff0_csv = os.path.join(data_path, 'diff0.csv')\n","diff1_csv = os.path.join(data_path, 'diff1.csv')\n","similar_csv = os.path.join(data_path, 'similar.csv')\n","\n","#read file\n","train_pos_file = os.path.join(data_path, \"train_nhan_0.txt\")\n","train_neg_file = os.path.join(data_path, \"train_nhan_1.txt\")\n","\n","test_pos_file = os.path.join(data_path, \"test_nhan_0.txt\")\n","test_neg_file = os.path.join(data_path, \"test_nhan_1.txt\")\n","test_file = os.path.join(data_path, \"test.txt\")\n","\n","\n","#save file\n","train_pos_save = os.path.join(data_path, \"train_pos\")\n","train_neg_save = os.path.join(data_path, \"train_neg\")\n","\n","test_pos_save = os.path.join(data_path, \"test_pos\")\n","test_neg_save = os.path.join(data_path, \"test_neg\")\n","test_save = os.path.join(data_path, \"test\")\n","\n","y_pred_save = os.path.join(data_path, 'y_pred.csv')\n","\n","word2vec_path = os.path.join(data_path, 'word2vec')\n","dataset = os.path.join(data_path, 'dataset.txt')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yOJQbOF_sWQ3","colab_type":"code","colab":{}},"cell_type":"code","source":["def load_raw_data(filename, is_train=True):\n","\n","    sample = []\n","    sample_list = []\n","\n","    regex = 'train_'\n","    if not is_train:\n","        regex = 'test_'\n","\n","    with open(filename, 'r') as file:\n","        for line in file :\n","            if regex in line:\n","                sample_list.append(sample)\n","                sample = [line]\n","            elif line!='\\n':\n","                sample.append(line)\n","\n","    sample_list.append(sample)      \n","\n","    return sample_list[1:]\n","\n","def save_csv(filename, csvfile, is_train=True):\n","    sample_list = load_raw_data(filename, is_train)\n","\n","    with open(csvfile, 'w') as writer_file:\n","        writer = csv.writer(writer_file)\n","        if is_train:\n","            writer.writerow(['id','comment','label'])\n","        elif not is_train:\n","            writer.writerow(['id', 'comment'])\n","        for sample in sample_list:\n","            id = sample[0].replace('\\n','')\n","            if is_train:\n","                comment = ' SPLIT '.join(sample[1:-1])\n","                comment = comment.replace('\\n','')\n","                label = sample[-1]\n","                writer.writerow([id, comment, label])\n","            else:\n","                comment = ' SPLIT '.join(sample[1:])\n","                comment = comment.replace('\\n','')\n","                writer.writerow([id, comment])\n","# save_csv(train_file, train_csv)\n","# save_csv(test_file, test_csv, is_train=False)\n","  \n","\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"BYjBmYrVuq4X","colab_type":"code","outputId":"737d1832-3dc8-43bb-e653-da2d5decc428","executionInfo":{"status":"ok","timestamp":1552274160135,"user_tz":-420,"elapsed":1178,"user":{"displayName":"Nghị Nguyễn Đình","photoUrl":"","userId":"02222397772048019559"}},"colab":{"base_uri":"https://localhost:8080/","height":336}},"cell_type":"code","source":["train_csv = os.path.join(data_path, \"train_correct_addTone.csv\")\n","train_df = pd.read_csv(train_csv)\n","train_df.info()  \n","train_df.head(5)\n","# comments = train_df['comment']\n","# labels = train_df['label']\n","# nhan_0_writer = open(train_pos_file, 'w')\n","# nhan_1_writer = open(train_neg_file, 'w')\n","# for i in range(len(labels)):\n","#     if labels[i] == 0:\n","#         nhan_0_writer.write(comments[i])\n","#     elif labels[i] == 1:\n","#         nhan_1_writer.write(comments[i])\n","#     else:\n","#         print('label: ', labels[i], ' - comment: ', comments[i])\n","    "],"execution_count":29,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 16087 entries, 0 to 16086\n","Data columns (total 3 columns):\n","id         16087 non-null object\n","comment    16087 non-null object\n","label      16087 non-null int64\n","dtypes: int64(1), object(2)\n","memory usage: 377.1+ KB\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>comment</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>train_000000</td>\n","      <td>dùng được sp tốt cảm ơn split shop đóng gói sả...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>train_000001</td>\n","      <td>chất lượng sản phẩm tuyệt vời sơn mịn nhưng kh...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>train_000002</td>\n","      <td>chất lượng sản phẩm tuyệt vời nhưng không có h...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>train_000003</td>\n","      <td>\":(( Mình hơi thất vọng 1 chút vì mình đã kỳ v...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>train_000004</td>\n","      <td>lần trước mình mua áo gió màu hồng rất ok mà đ...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             id                                            comment  label\n","0  train_000000  dùng được sp tốt cảm ơn split shop đóng gói sả...      0\n","1  train_000001  chất lượng sản phẩm tuyệt vời sơn mịn nhưng kh...      0\n","2  train_000002  chất lượng sản phẩm tuyệt vời nhưng không có h...      0\n","3  train_000003  \":(( Mình hơi thất vọng 1 chút vì mình đã kỳ v...      1\n","4  train_000004  lần trước mình mua áo gió màu hồng rất ok mà đ...      1"]},"metadata":{"tags":[]},"execution_count":29}]},{"metadata":{"id":"SFDkRcotAMDb","colab_type":"code","outputId":"9266e86f-2dfe-4c38-d9e6-3650433ce27b","executionInfo":{"status":"ok","timestamp":1552274161133,"user_tz":-420,"elapsed":1053,"user":{"displayName":"Nghị Nguyễn Đình","photoUrl":"","userId":"02222397772048019559"}},"colab":{"base_uri":"https://localhost:8080/","height":318}},"cell_type":"code","source":["test_csv = os.path.join(data_path, \"test_correct_addTone.csv\")\n","test_df = pd.read_csv(test_csv)\n","test_df.info()  \n","test_df.head(5)\n","# comments = test_df['comment']\n","# test_writer = open(test_file, 'w')\n","# for comment in comments:\n","#     test_writer.write(comment)\n"],"execution_count":30,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 10981 entries, 0 to 10980\n","Data columns (total 2 columns):\n","id         10981 non-null object\n","comment    10981 non-null object\n","dtypes: object(2)\n","memory usage: 171.6+ KB\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>comment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>test_000000</td>\n","      <td>chưa dùng thử nên chưa biết\\n</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>test_000001</td>\n","      <td>không đáng tiềnvì ngay đợt sale nên mới mua nh...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>test_000002</td>\n","      <td>cám ơn shop đóng gói sản phẩm rất đẹp và chắc ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>test_000003</td>\n","      <td>vải đẹpphom oki luônquá ưng\\n</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>test_000004</td>\n","      <td>chuẩn hàng đóng gói đẹp\\n</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            id                                            comment\n","0  test_000000                      chưa dùng thử nên chưa biết\\n\n","1  test_000001  không đáng tiềnvì ngay đợt sale nên mới mua nh...\n","2  test_000002  cám ơn shop đóng gói sản phẩm rất đẹp và chắc ...\n","3  test_000003                      vải đẹpphom oki luônquá ưng\\n\n","4  test_000004                          chuẩn hàng đóng gói đẹp\\n"]},"metadata":{"tags":[]},"execution_count":30}]},{"metadata":{"id":"pCBU9sEL5xyx","colab_type":"code","colab":{}},"cell_type":"code","source":["bad_word = {\"hơi thất vọng\", \"địt mẹ\", \"\"}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XNuJPg2AD6C8","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.8, max_features=10000, sublinear_tf=True, ngram_range=(1,3))\n","x_train, x_val, y_train, y_val = train_test_split(train_df.comment, train_df.label, test_size=0.3, random_state=42)\n","vectorizer.fit(x_train)\n","x_tfidf_train = vectorizer.transform(x_train)\n","x_tfidf_val = vectorizer.transform(x_val)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kr-1MWKe6Wav","colab_type":"code","colab":{}},"cell_type":"code","source":["# print(x_tfidf_train.shape)\n","# print(x_tfidf_val.shape)\n","# print(x_tfidf_train[1].shape)\n","# print('-----------------')\n","# print(x_tfidf_train[1])\n","# print('=================')\n","# print(x_tfidf_val[0])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VHoP125wGlyq","colab_type":"code","outputId":"8f4941f0-e080-48e2-94c8-e286e44016c7","executionInfo":{"status":"ok","timestamp":1551773585158,"user_tz":-420,"elapsed":1789,"user":{"displayName":"Nghị Nguyễn Đình","photoUrl":"","userId":"02222397772048019559"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression, RidgeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","model = LogisticRegression()\n","print(\"Train model........\")\n","sentiment_fit = model.fit(x_tfidf_train, y_train)\n","print(\"Predict...........\")\n","y_pred = sentiment_fit.predict(x_tfidf_val)\n","accuracy = accuracy_score(y_val, y_pred)\n","\n","with open(similar_csv, 'w') as fo:\n","    writer = csv.writer(fo)\n","    writer.writerow(['index', 'comment', 'label', 'predict'])\n","    df = pd.concat([x_val, y_val], axis=1)\n","    comment = x_val.values\n","    index = x_val.index\n","    label = y_val.values\n","    for i in range(len(comment)):\n","        if label[i] == y_pred[i]:\n","            writer.writerow([index[i], comment[i], label[i], y_pred[i]])\n","print(\"accuracy score: {0:.2f}%\".format(accuracy*100))\n","\n","x_tfidf_test = vectorizer.transform(test_df.comment)\n","y_predict = sentiment_fit.predict(x_tfidf_test)\n","test_df['label'] = y_predict\n","test_df[['id', 'label']].to_csv(sample_csv, index=False)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train model........\n","Predict...........\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["accuracy score: 89.89%\n"],"name":"stdout"}]},{"metadata":{"id":"cneiBMDVjGz4","colab_type":"code","outputId":"c9a9d111-a803-4843-9831-72c325628c1b","executionInfo":{"status":"ok","timestamp":1551671370630,"user_tz":-420,"elapsed":26892,"user":{"displayName":"Nghị Nguyễn Đình","photoUrl":"","userId":"02222397772048019559"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"cell_type":"code","source":["from sklearn import svm\n","from sklearn.metrics import accuracy_score\n","model = svm.SVC(kernel='linear')\n","sentiment_fit = model.fit(x_tfidf_train, y_train)\n","print(\"Predict...........\")\n","y_pred = sentiment_fit.predict(x_tfidf_val)\n","accuracy = accuracy_score(y_val, y_pred)\n","print(\"accuracy score: {0:.2f}%\".format(accuracy*100))\n","\n","x_tfidf_test = vectorizer.transform(test_df.comment)\n","y_predict = sentiment_fit.predict(x_tfidf_test)\n","test_df['label'] = y_predict\n","sample_csv = os.path.join(data_path, \"sample_svm.csv\")\n","test_df[['id', 'label']].to_csv(sample_csv, index=False)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Predict...........\n","accuracy score: 89.66%\n"],"name":"stdout"}]},{"metadata":{"id":"QzCbWNCgFp4n","colab_type":"code","colab":{}},"cell_type":"code","source":["# import emoji\n","\n","# def extract_emojis(str):\n","#     return [c for c in str if c in emoji.UNICODE_EMOJI]\n","\n","# good_df = train_df[train_df['label'] == 0]\n","# good_cmt = good_df['comment'].values\n","# good_emoji = []\n","# for cmt in good_gmt:\n","#     emoji = extract_emoji(cmt)\n","#     good_emoji.extend(emoji)\n","    \n","# good_emoji = np.unique(np.asarray(good_emoji))\n","\n","\n","# bad_df = train_df[train_df['label'] == 1]\n","# bad_cmt = bad_df['comment'].values\n","# bad_emoj = []\n","# for cmt in bad_cmt:\n","#     emoji = extract_emoji(cmt)\n","#     bad_emoji.extend(emoji)\n","\n","# bad_emoji = np.unique(np.asrray(bad_emoji))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"o0PdwRVBnJh6","colab_type":"code","colab":{}},"cell_type":"code","source":["# path = \"/content/drive/My Drive/AI_COLAB/machinelearningcoban-competition/SA_demo/pystacknet-master/setup.py\"\n","# print(path)\n","# !python \"/content/drive/My Drive/AI_COLAB/machinelearningcoban-competition/SA_demo/pystacknet-master/setup.py\" install"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GPeTK4CokQge","colab_type":"code","colab":{}},"cell_type":"code","source":["from pystacknet.pystacknet import StackNetClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import ExtraTreesClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","\n","models=[ \n","    ######## First level ########\n","    [\n","        RandomForestClassifier (n_estimators=100, criterion=\"entropy\", max_depth=5, max_features=0.5, random_state=1),\n","        ExtraTreesClassifier (n_estimators=100, criterion=\"entropy\", max_depth=5, max_features=0.5, random_state=1),\n","        GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, max_features=0.5, random_state=1),\n","        LogisticRegression(random_state=1)\n","    ],\n","    ######## Second level ########\n","    [\n","        RandomForestClassifier (n_estimators=200, criterion=\"entropy\", max_depth=5, max_features=0.5, random_state=1)\n","    ]\n","]\n","\n","\n","model = StackNetClassifier(\n","    models, metric=\"f1\", \n","    folds=5,\n","    restacking=False, \n","    use_retraining=True, \n","    use_proba=True, \n","    random_state=12345, n_jobs=1, verbose=1\n",")\n","\n","model.fit(x_tfidf_train, y_train)\n","# preds=model.predict_proba(x_tfidf_val)\n","# pred_cls = np.argmax(preds, axis=1)\n","# submission = pd.read_csv(\"./data/SA_demo/sample_submission.csv\")\n","# submission['label'] = pred_cls\n","# submission.to_csv(\"stack_demo.csv\", index=False)\n","\n","x_tfidf_test = vectorizer.transform(test_df.comment)\n","y_predict = model.predict(x_tfidf_test)\n","test_df['label'] = y_predict\n","sample_csv = os.path.join(data_path, \"sample_svm.csv\")\n","test_df[['id', 'label']].to_csv(sample_csv, index=False)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gP7dsDEFtaag","colab_type":"code","colab":{}},"cell_type":"code","source":["# from sklearn.model_selection import cross_val_predict\n","# from sklearn.ensemble import RandomForestClassifier\n","# from sklearn.ensemble import ExtraTreesClassifier\n","# from sklearn.ensemble import GradientBoostingClassifier\n","# from sklearn.model_selection import StratifiedKFold\n","\n","# models = [\n","#     RandomForestClassifier (n_estimators=100, criterion=\"entropy\", max_depth=5, max_features=0.5, random_state=1),\n","#     ExtraTreesClassifier (n_estimators=100, criterion=\"entropy\", max_depth=5, max_features=0.5, random_state=1),\n","#     GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, max_features=0.5, random_state=1),\n","#     LogisticRegression(random_state=1)\n","# ]\n","\n","# def cross_val_and_predict(clf, X, y, X_test, nfolds):\n","#     kf = StratifiedKFold(n_splits=nfolds, shuffle=True, random_state=42)\n","    \n","#     oof_preds = np.zeros((X.shape[0], 2))\n","#     sub_preds = np.zeros((X_test.shape[0], 2))\n","    \n","#     for fold, (train_idx, valid_idx) in enumerate(kf.split(X, y)):\n","#         X_train, y_train = X[train_idx], y[train_idx]\n","#         X_valid, y_valid = X[valid_idx], y[valid_idx]\n","        \n","#         print('===========================')\n","#         print(len(np.argwhere(np.isnan(y_train))))\n","#         print(np.any(np.isnan(y_train)))\n","#         print(np.all(np.isfinite(y_train)))\n","#         mask = np.all(np.isnan(y_train))\n","#         y_train[~mask]\n","#         print('===========================')\n","        \n","#         clf.fit(X_train, y_train)\n","        \n","#         oof_preds[valid_idx] = clf.predict_proba(X_valid)\n","#         sub_preds += clf.predict_proba(X_test) / kf.n_splits\n","        \n","#     return oof_preds, sub_preds\n","\n","# sub_preds = []\n","\n","# for clf in models:\n","#     oof_pred, sub_pred = cross_val_and_predict(clf, x_tfidf_train, y_train, x_tfidf_test, nfolds=5)\n","#     oof_pred_cls = oof_pred.argmax(axis=1)\n","#     oof_f1 = f1_score(y_pred=oof_pred_cls, y_true=y_train)\n","    \n","#     print(clf.__class__)\n","#     print(f\"F1 CV: {oof_f1}\")\n","    \n","#     sub_preds.append(sub_pred)\n","\n","# sub_preds = np.asarray(sub_preds)\n","# sub_preds = sub_preds.mean(axis=0)\n","# sub_pred_cls = sub_preds.argmax(axis=1)\n","\n","# submission_ensemble = submission.copy()\n","# submission_ensemble['label'] = sub_pred_cls\n","# submission_ensemble.to_csv(\"ensemble.csv\", index=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uAqO6X0FyPZu","colab_type":"code","outputId":"03d56616-28c3-4289-de56-5543a492dbaa","executionInfo":{"status":"ok","timestamp":1552274270829,"user_tz":-420,"elapsed":5660,"user":{"displayName":"Nghị Nguyễn Đình","photoUrl":"","userId":"02222397772048019559"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# Keras\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n","from keras.layers.embeddings import Embedding\n","## Plotly\n","import plotly.offline as py\n","import plotly.graph_objs as go\n","py.init_notebook_mode(connected=True)\n","# Others\n","import nltk\n","import string\n","import numpy as np\n","import pandas as pd\n","from nltk.corpus import stopwords\n","\n","from sklearn.manifold import TSNE\n","\n","### Create sequence\n","vocabulary_size = 10000\n","tokenizer = Tokenizer(num_words= vocabulary_size)\n","tokenizer.fit_on_texts(train_df['comment'])\n","train_sequences = tokenizer.texts_to_sequences(train_df['comment'])\n","test_sequences = tokenizer.texts_to_sequences(test_df['comment'])\n","x_train = pad_sequences(train_sequences, maxlen=50)\n","x_test = pad_sequences(test_sequences, maxlen=50)\n","labels = train_df['label']\n","\n","\n","def create_conv_model():\n","    model_conv = Sequential()\n","    model_conv.add(Embedding(vocabulary_size, 100, input_length=50))\n","#     model_conv.add(Dropout(0.2))\n","#     model_conv.add(Conv1D(64, 5, activation='relu'))\n","#     model_conv.add(MaxPooling1D(pool_size=4))\n","    model_conv.add(LSTM(100))\n","    model_conv.add(Dense(1, activation='sigmoid'))\n","    model_conv.compile(loss='binary_crossentropy', optimizer='adam',    metrics=['accuracy'])\n","    return model_conv\n","# model_conv = create_conv_model()\n","# model_conv.fit(x_train, np.array(labels), validation_split=0.4, epochs = 100)\n"],"execution_count":34,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"],"text/vnd.plotly.v1+html":"<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"},"metadata":{"tags":[]}}]},{"metadata":{"id":"g9Q7D0EO1gFa","colab_type":"code","outputId":"e0bf658e-48ff-40b9-c7ef-9cf11332117c","executionInfo":{"status":"ok","timestamp":1551694416700,"user_tz":-420,"elapsed":7132,"user":{"displayName":"Nghị Nguyễn Đình","photoUrl":"","userId":"02222397772048019559"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"cell_type":"code","source":["# evaluate the model\n","# scores = model.evaluate(X, Y, verbose=0)\n","# print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n"," \n","# serialize model to JSON\n","model_json = model_conv.to_json()\n","with open(\"model.json\", \"w\") as json_file:\n","    json_file.write(model_json)\n","# serialize weights to HDF5\n","model_conv.save_weights(\"model.h5\")\n","print(\"Saved model to disk\")\n"," \n","# later...\n"," \n","from keras.models import model_from_json\n","# load json and create model\n","json_file = open('model.json', 'r')\n","loaded_model_json = json_file.read()\n","json_file.close()\n","loaded_model = model_from_json(loaded_model_json)\n","# load weights into new model\n","loaded_model.load_weights(\"model.h5\")\n","print(\"Loaded model from disk\")\n","\n","\n","y_predict = model_conv.predict_classes(x_test)\n","test_df['label'] = y_predict\n","sample_csv = os.path.join(data_path, \"sample_lstm.csv\")\n","print('sample: ', sample_csv)\n","test_df[['id', 'label']].to_csv(sample_csv, index=False)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Saved model to disk\n","Loaded model from disk\n","sample:  /content/drive/My Drive/AI_COLAB/machinelearningcoban-competition/SA_demo/sample_lstm.csv\n"],"name":"stdout"}]},{"metadata":{"id":"hBDhM_7PBNKh","colab_type":"code","colab":{}},"cell_type":"code","source":["embeddings_index = dict()\n","with open(os.path.join(data_path, 'glove.6B.100d.txt')) as fo:\n","    for line in fo:\n","        values = line.split()\n","        word = values[0]\n","        coefs = np.asarray(values[1:], dtype='float32')\n","        embeddings_index[word] = coefs\n","    fo.close()\n","         \n","         \n","embedding_matrix = np.zeros((vocabulary_size, 100))\n","for word, index in tokenizer.word_index.items():\n","    if index > vocabulary_size - 1:\n","        break\n","    else:\n","        embedding_vector = embeddings_index.get(word)\n","        if embedding_vector is not None:\n","            embedding_matrix[index] = embedding_vector\n","         \n","## create model\n","model_glove = Sequential()\n","model_glove.add(Embedding(vocabulary_size, 100, input_length=50, weights=[embedding_matrix], trainable=False))\n","model_glove.add(Dropout(0.2))\n","model_glove.add(Conv1D(64, 5, activation='relu'))\n","model_glove.add(MaxPooling1D(pool_size=4))\n","model_glove.add(LSTM(100))\n","model_glove.add(Dense(1, activation='sigmoid'))\n","model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","## Fit train data\n","model_glove.fit(x_train, np.array(labels), validation_split=0.4, epochs = 3)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UWvkdA2L-iRm","colab_type":"code","outputId":"438dea57-629c-4d9a-f3e3-10194450ee0f","executionInfo":{"status":"ok","timestamp":1552272125264,"user_tz":-420,"elapsed":4769,"user":{"displayName":"Nghị Nguyễn Đình","photoUrl":"","userId":"02222397772048019559"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"cell_type":"code","source":["!pip install fasttext"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: fasttext in /usr/local/lib/python2.7/dist-packages (0.8.3)\n","Requirement already satisfied: future in /usr/local/lib/python2.7/dist-packages (from fasttext) (0.16.0)\n","Requirement already satisfied: numpy>=1 in /usr/local/lib/python2.7/dist-packages (from fasttext) (1.14.6)\n"],"name":"stdout"}]},{"metadata":{"id":"s3t_JM2HoQFq","colab_type":"code","colab":{}},"cell_type":"code","source":["import fasttext\n","word2vec_model = fasttext.cbow(dataset, word2vec_path)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"B0qREmQ5pDUV","colab_type":"code","colab":{}},"cell_type":"code","source":["word2vec_model = fasttext.load_model(word2vec_path + \".bin\")\n","\n","### Create sequence\n","vocabulary_size = 16000\n","embedding_size = 100\n","tokenizer = Tokenizer(num_words= vocabulary_size)\n","tokenizer.fit_on_texts(train_df['comment'])\n","  \n","embedding_matrix = np.zeros((vocabulary_size, embedding_size))\n","for word, index in tokenizer.word_index.items():\n","    if index > vocabulary_size - 1:\n","        break\n","    else:\n","        embedding_vector = word2vec_model[word]\n","        if embedding_vector is not None:\n","            embedding_matrix[index] = embedding_vector\n","         \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9jFpYsylZbva","colab_type":"code","outputId":"4c260730-e992-41b9-f743-294e27988bf1","executionInfo":{"status":"ok","timestamp":1552274334677,"user_tz":-420,"elapsed":6649,"user":{"displayName":"Nghị Nguyễn Đình","photoUrl":"","userId":"02222397772048019559"}},"colab":{"base_uri":"https://localhost:8080/","height":175}},"cell_type":"code","source":["import numpy as np\n","import re\n","import numpy\n","\n","\t\n","word2id= {}\n","id2word={}\n","index = 1\n","maxlen = 0\n","avglen = 0\n","count100 = 0\n","\n","open_files = [train_pos_file, train_neg_file, test_file]\n","\n","save_files = [train_pos_save, train_neg_save, test_save]\n","\n","for open_file, save_file in zip(open_files,save_files):\n","    pos = []\n","    file = open(open_file, 'r')\n","\n","    for aline in file.readlines():\n","        aline = aline.replace('\\n', \"\")\n","        ids = np.array([], dtype='int32')\n","        for word in aline.split(' '):\n","            word = word.lower()\n","            if word in word2id:\n","                ids = np.append(ids, word2id[word])\n","            else:\n","                if word != '':\n","                    # print (word, \"not in vocalbulary\")\n","                    word2id[word] = index\n","                    id2word[index] = word\n","                    ids = np.append(ids, index)\n","                    index = index + 1\n","        if len(ids) > 0:\n","            pos.append(ids)\n","\n","    file.close()\n","    print(('len pos: ', len(pos)))\n","    np.save(save_file, pos)\n","    for li in pos:\n","        if maxlen < len(li):\n","            maxlen = len(li)\n","        avglen += len(li)\n","        if len(li) > 250:\n","            count100+=1\n","    print('avg len: ', avglen / len(pos))\n","\n","print(('word2id: ', len(word2id)))\n","print((\"maxlen\",maxlen))\n","print((\"maxlen250\",count100))\n"],"execution_count":37,"outputs":[{"output_type":"stream","text":["('len pos: ', 9280)\n","('avg len: ', 18)\n","('len pos: ', 6807)\n","('avg len: ', 49)\n","('len pos: ', 10981)\n","('avg len: ', 51)\n","('word2id: ', 15706)\n","('maxlen', 639)\n","('maxlen250', 30)\n"],"name":"stdout"}]},{"metadata":{"id":"lynBzBOpoBW_","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import os\n","import pickle\n","import math\n","# import cPickle\n","\n","def load_data_shuffle():\n","    train_pos_save = os.path.join(data_path, \"train_pos.npy\")\n","    train_neg_save = os.path.join(data_path, \"train_neg.npy\")\n","    #Load train data\n","    pos_train = np.load(train_pos_save,encoding='ASCII')\n","    neg_train = np.load(train_neg_save,encoding='ASCII')\n","\n","    y_pos_train = []\n","    for i in pos_train:\n","        y_pos_train.append([0,1])\n","    y_pos_train = np.array(y_pos_train)\n","\n","    y_neg_train = []\n","    for i in neg_train:\n","        y_neg_train.append([1, 0])\n","    y_neg_train = np.array(y_neg_train)\n","    \n","#     print('pos train shape: ', pos_train.shape)\n","    print(pos_train)\n","    #split train and validate set\n","    val_len = int(round(len(pos_train)/10))\n","#     print('val_len: ', val_len)\n","\n","    pos_val = pos_train[0:val_len]\n","    pos_train = pos_train[val_len:]\n","    y_pos_val = y_pos_train[0:val_len]\n","    y_pos_train = y_pos_train[val_len:]\n","\n","\n","    neg_val = neg_train[0:val_len]\n","    neg_train = neg_train[val_len:]\n","    y_neg_val = y_neg_train[0:val_len]\n","    y_neg_train = y_neg_train[val_len:]\n","\n","\n","    X_train = np.concatenate([pos_train,neg_train])\n","    y_train = np.concatenate([y_pos_train,y_neg_train])\n","\n","    X_val = np.concatenate([pos_val,neg_val])\n","    y_val = np.concatenate([y_pos_val,y_neg_val])\n","\n","    test_save = os.path.join(data_path, \"test.npy\")\n","    X_test = np.load(test_save, encoding='ASCII')\n","#     X_test = np.concatenate([pos_test,neu_test,neg_test])\n","#     y_test = np.concatenate([y_pos_test,y_neu_test,y_neg_test])\n","\n","    # print X_train.shape, y_train.shape\n","    # print X_test.shape, y_test.shape\n","    # print X_val.shape, y_val.shape\n","\n","    return X_train, y_train, X_val, y_val, X_test\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9mL3D4B71u5T","colab_type":"code","outputId":"65b62b0f-21b8-4d1b-e81a-51e3472e72f3","executionInfo":{"status":"ok","timestamp":1552065580777,"user_tz":-420,"elapsed":4360334,"user":{"displayName":"Nghị Nguyễn Đình","photoUrl":"","userId":"02222397772048019559"}},"colab":{"base_uri":"https://localhost:8080/","height":1055}},"cell_type":"code","source":["'''This example demonstrates the use of Convolution1D for text classification.\n","Gets to 0.88 test accuracy after 2 epochs.\n","90s/epoch on Intel i5 2.4Ghz CPU.\n","10s/epoch on Tesla K40 GPU.\n","'''\n","\n","from __future__ import print_function\n","import numpy as np\n","import pickle\n","\n","\n","np.random.seed(1337)  # for reproducibility\n","\n","from keras.preprocessing import sequence\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation, Lambda\n","from keras.layers import Embedding\n","from keras.layers import Convolution1D, LSTM\n","from keras.datasets import imdb\n","from keras import backend as K\n","from keras.optimizers import Adadelta\n","from keras.preprocessing import sequence as sq\n","from keras.layers import Dense, Dropout, Activation, Lambda,merge,Input,TimeDistributed,Flatten, Add\n","from keras.models import Model\n","from keras.callbacks import ModelCheckpoint\n","\n","import keras.backend.tensorflow_backend as K\n","\n","#config = K.tf.ConfigProto(intra_op_parallelism_threads=16, inter_op_parallelism_threads=16, \\\n","#                        allow_soft_placement=True, device_count = {'CPU': 1})\n","\n","\n","# tf_config = K.tf.ConfigProto()\n","# tf_config.gpu_options.allow_growth = True\n","# session = K.tf.Session(config=tf_config)\n","# K.set_session(session)\n","\n","# config = K.tf.ConfigProto(intra_op_parallelism_threads=4, inter_op_parallelism_threads=4, \\\n","#                         allow_soft_placement=True, device_count = {'CPU': 4})\n","# session = K.tf.Session(config=config)\n","# K.set_session(session)\n","\n","# set parameters:\n","max_features = 16000#14300\n","maxlen = 40\n","batch_size = 32\n","embedding_dims = 100\n","nb_filter = 150\n","filter_length = 3\n","hidden_dims = 100\n","nb_epoch = 50\n","\n","accs = []\n","print('Loading data ...', )\n","\n","X_train, y_train, X_val, y_val, X_test = load_data_shuffle()\n","print(len(X_train), 'train sequences')\n","print(len(X_test), 'test sequences')\n","\n","X_train = sq.pad_sequences(X_train, maxlen=maxlen, padding='pre', truncating='post', value=0.0)\n","X_test = sq.pad_sequences(X_test, maxlen=maxlen, padding='pre', truncating='post', value=0.0)\n","X_val = sq.pad_sequences(X_val, maxlen=maxlen, padding='pre', truncating='post', value=0.0)\n","print('X_train shape:', X_train.shape)\n","print('X_val shape:', X_val.shape)\n","print('X_test shape:', X_test.shape)\n","\n","print('Build model...')\n","model = Sequential()\n","\n","input_layer = Input(shape=(maxlen,),dtype='int32', name='main_input')\n","emb_layer = Embedding(max_features,\n","                      embedding_dims,\n","                      input_length=maxlen,\n","                      weights=[embedding_matrix],\n","                      trainable=False\n","                      )(input_layer)\n","def max_1d(X):\n","    return K.max(X, axis=1)\n","\n","# we add a Convolution1D, which will learn nb_filter\n","# word group filters of size 3:\n","\n","con3_layer = Convolution1D(nb_filter=nb_filter,\n","                    filter_length=3,\n","                    border_mode='valid',\n","                    activation='relu',\n","                    subsample_length=1)(emb_layer)\n","\n","pool_con3_layer = Lambda(max_1d, output_shape=(nb_filter,))(con3_layer)\n","\n","\n","# we add a Convolution1D, which will learn nb_filter\n","# word group filters of size 4:\n","\n","con4_layer = Convolution1D(nb_filter=nb_filter,\n","                    filter_length=5,\n","                    border_mode='valid',\n","                    activation='relu',\n","                    subsample_length=1)(emb_layer)\n","\n","pool_con4_layer = Lambda(max_1d, output_shape=(nb_filter,))(con4_layer)\n","\n","\n","# we add a Convolution1D, which will learn nb_filter\n","# word group filters of size 5:\n","\n","con5_layer = Convolution1D(nb_filter=nb_filter,\n","                    filter_length=7,\n","                    border_mode='valid',\n","                    activation='relu',\n","                    subsample_length=1)(emb_layer)\n","\n","pool_con5_layer = Lambda(max_1d, output_shape=(nb_filter,))(con5_layer)\n","\n","cnn_layer = Add()([pool_con3_layer, pool_con5_layer,pool_con4_layer])\n","print(pool_con3_layer.shape)\n","print(pool_con5_layer.shape)\n","print(pool_con4_layer.shape)\n","print(cnn_layer.shape)\n","\n","# cnn_layer = merge([pool_con3_layer, pool_con5_layer,pool_con4_layer ], mode='concat')\n","\n","\n","#LSTM\n","\n","\n","x = Embedding(max_features, embedding_dims, input_length=maxlen)(input_layer)\n","lstm_layer = LSTM(150)(x)\n","\n","cnn_lstm_layer = Add()([lstm_layer, cnn_layer])\n","# cnn_lstm_layer = merge([lstm_layer, cnn_layer], mode='concat')\n","\n","dense_layer = Dense(hidden_dims*2, activation='sigmoid')(cnn_lstm_layer)\n","output_layer= Dropout(0.2)(dense_layer)\n","output_layer = Dense(2, trainable=True, activation='sigmoid')(output_layer)\n","\n","\n","\n","\n","model = Model(input=[input_layer], output=[output_layer])\n","adadelta = Adadelta(lr=0.5, rho=0.95, epsilon=1e-06)\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=\"adamax\",\n","              metrics=['accuracy'])\n","checkpoint = ModelCheckpoint(os.path.join(data_path, 'weights.hdf5'),\n","                             monitor='val_acc', verbose=0, save_best_only=True,\n","                             mode='max')\n","model.fit(X_train, y_train,\n","          batch_size=batch_size,\n","          nb_epoch=nb_epoch,\n","          callbacks=[checkpoint],\n","          validation_data=(X_val, y_val))\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loading data ...\n","[array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n","       18, 19, 11, 12, 20, 21])\n"," array([18, 19, 11, 12, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32])\n"," array([18, 19, 11, 12, 20, 21, 24, 28, 33, 34, 28, 33, 35, 36, 37, 28, 33,\n","       38])\n"," ... array([ 53,  14, 145,  78, 291])\n"," array([301, 302,  67,  68,  69,  70,   9,  10,  11,  12,  13,  14,  15,\n","        16,  17, 167, 168,  81,  53,  13,  54,   8,  64,  65,  13,   4])\n"," array([172, 173, 175,  14])]\n","14231 train sequences\n","10981 test sequences\n","X_train shape: (14231, 40)\n","X_val shape: (1856, 40)\n","X_test shape: (10981, 40)\n","Build model...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:87: UserWarning:\n","\n","Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", padding=\"valid\", strides=1, filters=150, kernel_size=3)`\n","\n","/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:99: UserWarning:\n","\n","Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", padding=\"valid\", strides=1, filters=150, kernel_size=5)`\n","\n","/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:111: UserWarning:\n","\n","Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", padding=\"valid\", strides=1, filters=150, kernel_size=7)`\n","\n"],"name":"stderr"},{"output_type":"stream","text":["(?, 150)\n","(?, 150)\n","(?, 150)\n","(?, 150)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:140: UserWarning:\n","\n","Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=[<tf.Tenso...)`\n","\n","/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:153: UserWarning:\n","\n","The `nb_epoch` argument in `fit` has been renamed `epochs`.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 14231 samples, validate on 1856 samples\n","Epoch 1/50\n","14231/14231 [==============================] - 47s 3ms/step - loss: 0.4051 - acc: 0.7999 - val_loss: 0.3025 - val_acc: 0.8820\n","Epoch 2/50\n","14231/14231 [==============================] - 45s 3ms/step - loss: 0.2756 - acc: 0.8890 - val_loss: 0.2680 - val_acc: 0.8917\n","Epoch 3/50\n","14231/14231 [==============================] - 45s 3ms/step - loss: 0.2392 - acc: 0.9075 - val_loss: 0.2601 - val_acc: 0.8874\n","Epoch 4/50\n","14231/14231 [==============================] - 45s 3ms/step - loss: 0.2027 - acc: 0.9215 - val_loss: 0.2588 - val_acc: 0.8960\n","Epoch 5/50\n","14231/14231 [==============================] - 45s 3ms/step - loss: 0.1726 - acc: 0.9356 - val_loss: 0.2777 - val_acc: 0.8922\n","Epoch 6/50\n","14231/14231 [==============================] - 45s 3ms/step - loss: 0.1379 - acc: 0.9488 - val_loss: 0.3001 - val_acc: 0.9025\n","Epoch 7/50\n","14231/14231 [==============================] - 45s 3ms/step - loss: 0.1101 - acc: 0.9595 - val_loss: 0.3593 - val_acc: 0.8809\n","Epoch 8/50\n","14231/14231 [==============================] - 45s 3ms/step - loss: 0.0847 - acc: 0.9706 - val_loss: 0.3627 - val_acc: 0.8842\n","Epoch 9/50\n","14080/14231 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9744"],"name":"stdout"}]},{"metadata":{"id":"ZnMc8JjC53A1","colab_type":"code","outputId":"9c2eed6a-dcd0-4166-f449-09316d2b955c","executionInfo":{"status":"ok","timestamp":1552066080641,"user_tz":-420,"elapsed":16998,"user":{"displayName":"Nghị Nguyễn Đình","photoUrl":"","userId":"02222397772048019559"}},"colab":{"base_uri":"https://localhost:8080/","height":311}},"cell_type":"code","source":["model.load_weights(os.path.join(data_path, 'weights.hdf5'))\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=\"adamax\",\n","              metrics=['accuracy'])\n","score, acc = model.evaluate(X_val, y_val, batch_size=batch_size)\n","y_predict = model.predict(X_test, batch_size=batch_size)\n","cls = []\n","print('shape: ', y_predict.shape)\n","print(y_predict[0])\n","print('type: ', type(y_predict[0]))\n","print('shape: ', y_predict[0].shape)\n","for x in y_predict[0]:\n","    print(x)\n","for i in range(len(y_predict)):\n","    if y_predict[i][0] < y_predict[i][1]: \n","        cls.append(0)\n","    else: cls.append(1)\n","print(y_predict)\n","test_df['label'] = np.asarray(cls)\n","sample_csv = os.path.join(data_path, \"sample_cnn-lstm.csv\")\n","test_df[['id', 'label']].to_csv(sample_csv, index=False)\n","\n","print(\"Acc:\", acc)\n","print(\"Score: \", score)\n","# accs.append(acc)\n","# print (\"Average Acc:\", K.np.mean(K.np.array(accs)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1856/1856 [==============================] - 2s 1ms/step\n","shape:  (10981, 2)\n","[0.01369217 0.5331809 ]\n","type:  <type 'numpy.ndarray'>\n","shape:  (2,)\n","0.01369217\n","0.5331809\n","[[0.01369217 0.5331809 ]\n"," [0.8669045  0.01974663]\n"," [0.00330365 0.4797629 ]\n"," ...\n"," [0.06089452 0.2589154 ]\n"," [0.00212303 0.57569563]\n"," [0.03346317 0.2967253 ]]\n","Acc: 0.9094827586206896\n","Score:  0.2424460472860213\n"],"name":"stdout"}]},{"metadata":{"id":"ONSXYFMj_tyI","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}