{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"Ck1xRjJ7XjjV","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eY1sz8AxkS4u","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"VySZcER4X2Nf","colab_type":"code","outputId":"0a721656-b13c-48b7-d197-a0ab6290a889","executionInfo":{"status":"ok","timestamp":1552268260943,"user_tz":-420,"elapsed":8254,"user":{"displayName":"Nghị Nguyễn Đình","photoUrl":"","userId":"02222397772048019559"}},"colab":{"base_uri":"https://localhost:8080/","height":90}},"cell_type":"code","source":["# http://pytorch.org/\n","from os.path import exists\n","from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n","accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n","!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n","import torch"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\u001b[31m  HTTP error 403 while getting http://download.pytorch.org/whl/cu100/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\u001b[0m\n","\u001b[31m  Could not install requirement torch==0.4.1 from http://download.pytorch.org/whl/cu100/torch-0.4.1-cp36-cp36m-linux_x86_64.whl because of error 403 Client Error: Forbidden for url: http://download.pytorch.org/whl/cu100/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\u001b[0m\n","\u001b[31mCould not install requirement torch==0.4.1 from http://download.pytorch.org/whl/cu100/torch-0.4.1-cp36-cp36m-linux_x86_64.whl because of HTTP error 403 Client Error: Forbidden for url: http://download.pytorch.org/whl/cu100/torch-0.4.1-cp36-cp36m-linux_x86_64.whl for URL http://download.pytorch.org/whl/cu100/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\u001b[0m\n"],"name":"stdout"}]},{"metadata":{"id":"vHfdnc3NX3Cw","colab_type":"code","outputId":"192a27ca-d290-4ad5-efc9-4fa9eadcd012","executionInfo":{"status":"ok","timestamp":1552268507148,"user_tz":-420,"elapsed":979,"user":{"displayName":"Nghị Nguyễn Đình","photoUrl":"","userId":"02222397772048019559"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["print(accelerator)\n","path = \"/content/drive/My Drive/AI_COLAB/Colab Notebooks/\"\n","USE_CUDA = torch.cuda.is_available()\n","print('cuda: ', USE_CUDA)\n","device = torch.device('cuda' if USE_CUDA else 'cpu')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["cu100\n","cuda:  True\n"],"name":"stdout"}]},{"metadata":{"id":"E1SaExMLa5x9","colab_type":"text"},"cell_type":"markdown","source":["import lib"]},{"metadata":{"id":"9zU9lzFyX7Dh","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","import pandas as pd\n","import csv\n","import numpy as np"],"execution_count":0,"outputs":[]},{"metadata":{"id":"N0P0UFHVa7mW","colab_type":"text"},"cell_type":"markdown","source":["Path to directory"]},{"metadata":{"id":"irkfrL6yX83R","colab_type":"code","colab":{}},"cell_type":"code","source":["path = \"/content/drive/My Drive/AI_COLAB/machinelearningcoban-competition/\"\n","data_path = os.path.join(path, \"SA_demo\")\n","train_file = os.path.join(data_path, \"train.crash\")\n","test_file = os.path.join(data_path, \"test.crash\")\n","train_csv = os.path.join(data_path, \"train.csv\")\n","test_csv = os.path.join(data_path, \"test.csv\")\n","sample_csv = os.path.join(data_path, \"sample.csv\")\n","compare_csv = os.path.join(data_path, \"compare.csv\")\n","diff_csv = os.path.join(data_path, 'diff.csv')\n","diff0_csv = os.path.join(data_path, 'diff0.csv')\n","diff1_csv = os.path.join(data_path, 'diff1.csv')\n","similar_csv = os.path.join(data_path, 'similar.csv')\n","\n","#read file\n","train_pos_file = os.path.join(data_path, \"train_nhan_0.txt\")\n","train_neg_file = os.path.join(data_path, \"train_nhan_1.txt\")\n","\n","test_pos_file = os.path.join(data_path, \"test_nhan_0.txt\")\n","test_neg_file = os.path.join(data_path, \"test_nhan_1.txt\")\n","test_file = os.path.join(data_path, \"test.txt\")\n","\n","\n","#save file\n","train_pos_save = os.path.join(data_path, \"train_pos\")\n","train_neg_save = os.path.join(data_path, \"train_neg\")\n","\n","test_pos_save = os.path.join(data_path, \"test_pos\")\n","test_neg_save = os.path.join(data_path, \"test_neg\")\n","test_save = os.path.join(data_path, \"test\")\n","\n","y_pred_save = os.path.join(data_path, 'y_pred.csv')\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"k5MN7Mk4bBYR","colab_type":"text"},"cell_type":"markdown","source":["Load raw data and convert to csv file"]},{"metadata":{"id":"HM0pUibwX-bZ","colab_type":"code","colab":{}},"cell_type":"code","source":["def load_raw_data(filename, is_train=True):\n","\n","    sample = []\n","    sample_list = []\n","\n","    regex = 'train_'\n","    if not is_train:\n","        regex = 'test_'\n","\n","    with open(filename, 'r') as file:\n","        for line in file :\n","            if regex in line:\n","                sample_list.append(sample)\n","                sample = [line]\n","            elif line!='\\n':\n","                sample.append(line)\n","\n","    sample_list.append(sample)      \n","\n","    return sample_list[1:]\n","\n","def save_csv(filename, csvfile, is_train=True):\n","    sample_list = load_raw_data(filename, is_train)\n","\n","    with open(csvfile, 'w') as writer_file:\n","        writer = csv.writer(writer_file)\n","        if is_train:\n","            writer.writerow(['id','comment','label'])\n","        elif not is_train:\n","            writer.writerow(['id', 'comment'])\n","        for sample in sample_list:\n","            id = sample[0].replace('\\n','')\n","            if is_train:\n","                comment = ' SPLIT '.join(sample[1:-1])\n","                comment = comment.replace('\\n','')\n","                label = sample[-1]\n","                writer.writerow([id, comment, label])\n","            else:\n","                comment = ' SPLIT '.join(sample[1:])\n","                comment = comment.replace('\\n','')\n","                writer.writerow([id, comment])\n","# save_csv(train_file, train_csv)\n","# save_csv(test_file, test_csv, is_train=False)\n","  \n","\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"UCAVAa7XbGYs","colab_type":"text"},"cell_type":"markdown","source":["Traing dataframe"]},{"metadata":{"id":"SB4R8MZeYAoi","colab_type":"code","outputId":"66fb461a-fc12-416a-eb34-d798d893022f","executionInfo":{"status":"ok","timestamp":1552267905621,"user_tz":-420,"elapsed":23780,"user":{"displayName":"Nghị Nguyễn Đình","photoUrl":"","userId":"02222397772048019559"}},"colab":{"base_uri":"https://localhost:8080/","height":336}},"cell_type":"code","source":["train_csv = os.path.join(data_path, \"train_correct_addTone.csv\")\n","train_df = pd.read_csv(train_csv)\n","train_df.info()  \n","train_df.head(5)\n","# comments = train_df['comment']\n","# labels = train_df['label']\n","# nhan_0_writer = open(train_pos_file, 'w')\n","# nhan_1_writer = open(train_neg_file, 'w')\n","# for i in range(len(labels)):\n","#     if labels[i] == 0:\n","#         nhan_0_writer.write(comments[i])\n","#     elif labels[i] == 1:\n","#         nhan_1_writer.write(comments[i])\n","#     else:\n","#         print('label: ', labels[i], ' - comment: ', comments[i])\n","    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 16087 entries, 0 to 16086\n","Data columns (total 3 columns):\n","id         16087 non-null object\n","comment    16087 non-null object\n","label      16087 non-null int64\n","dtypes: int64(1), object(2)\n","memory usage: 377.1+ KB\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>comment</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>train_000000</td>\n","      <td>dùng được sp tốt cảm ơn split shop đóng gói sả...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>train_000001</td>\n","      <td>chất lượng sản phẩm tuyệt vời sơn mịn nhưng kh...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>train_000002</td>\n","      <td>chất lượng sản phẩm tuyệt vời nhưng không có h...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>train_000003</td>\n","      <td>\":(( Mình hơi thất vọng 1 chút vì mình đã kỳ v...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>train_000004</td>\n","      <td>lần trước mình mua áo gió màu hồng rất ok mà đ...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             id                                            comment  label\n","0  train_000000  dùng được sp tốt cảm ơn split shop đóng gói sả...      0\n","1  train_000001  chất lượng sản phẩm tuyệt vời sơn mịn nhưng kh...      0\n","2  train_000002  chất lượng sản phẩm tuyệt vời nhưng không có h...      0\n","3  train_000003  \":(( Mình hơi thất vọng 1 chút vì mình đã kỳ v...      1\n","4  train_000004  lần trước mình mua áo gió màu hồng rất ok mà đ...      1"]},"metadata":{"tags":[]},"execution_count":8}]},{"metadata":{"id":"F2DzhXHIbIqN","colab_type":"text"},"cell_type":"markdown","source":["Test dataframe"]},{"metadata":{"id":"h0LB5GCFYC0A","colab_type":"code","outputId":"44e0bc98-b1fe-45d9-e010-99344bea5831","executionInfo":{"status":"ok","timestamp":1552267905979,"user_tz":-420,"elapsed":17176,"user":{"displayName":"Nghị Nguyễn Đình","photoUrl":"","userId":"02222397772048019559"}},"colab":{"base_uri":"https://localhost:8080/","height":318}},"cell_type":"code","source":["test_csv = os.path.join(data_path, \"test_correct_addTone.csv\")\n","test_df = pd.read_csv(test_csv)\n","test_df.info()  \n","test_df.head(5)\n","# comments = test_df['comment']\n","# test_writer = open(test_file, 'w')\n","# for comment in comments:\n","#     test_writer.write(comment)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 10981 entries, 0 to 10980\n","Data columns (total 2 columns):\n","id         10981 non-null object\n","comment    10981 non-null object\n","dtypes: object(2)\n","memory usage: 171.7+ KB\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>comment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>test_000000</td>\n","      <td>chưa dùng thử nên chưa biết\\n</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>test_000001</td>\n","      <td>không đáng tiềnvì ngay đợt sale nên mới mua nh...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>test_000002</td>\n","      <td>cám ơn shop đóng gói sản phẩm rất đẹp và chắc ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>test_000003</td>\n","      <td>vải đẹpphom oki luônquá ưng\\n</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>test_000004</td>\n","      <td>chuẩn hàng đóng gói đẹp\\n</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            id                                            comment\n","0  test_000000                      chưa dùng thử nên chưa biết\\n\n","1  test_000001  không đáng tiềnvì ngay đợt sale nên mới mua nh...\n","2  test_000002  cám ơn shop đóng gói sản phẩm rất đẹp và chắc ...\n","3  test_000003                      vải đẹpphom oki luônquá ưng\\n\n","4  test_000004                          chuẩn hàng đóng gói đẹp\\n"]},"metadata":{"tags":[]},"execution_count":9}]},{"metadata":{"id":"44_L1MhnYHmg","colab_type":"text"},"cell_type":"markdown","source":["Extrace tf-idf feature"]},{"metadata":{"id":"pE3vyMWZYEgw","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.8, max_features=10000, sublinear_tf=True, ngram_range=(1,3))\n","x_train, x_val, y_train, y_val = train_test_split(train_df.comment, train_df.label, test_size=0.3, random_state=42)\n","vectorizer.fit(x_train)\n","x_tfidf_train = vectorizer.transform(x_train)\n","x_tfidf_val = vectorizer.transform(x_val)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5aaqKOAvYSpV","colab_type":"text"},"cell_type":"markdown","source":["Logistic classification"]},{"metadata":{"id":"JfvdbrmbYNQt","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression, RidgeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","model = LogisticRegression()\n","print(\"Train model........\")\n","sentiment_fit = model.fit(x_tfidf_train, y_train)\n","print(\"Predict...........\")\n","y_pred = sentiment_fit.predict(x_tfidf_val)\n","accuracy = accuracy_score(y_val, y_pred)\n","\n","with open(similar_csv, 'w') as fo:\n","    writer = csv.writer(fo)\n","    writer.writerow(['index', 'comment', 'label', 'predict'])\n","    df = pd.concat([x_val, y_val], axis=1)\n","    comment = x_val.values\n","    index = x_val.index\n","    label = y_val.values\n","    for i in range(len(comment)):\n","        if label[i] == y_pred[i]:\n","            writer.writerow([index[i], comment[i], label[i], y_pred[i]])\n","print(\"accuracy score: {0:.2f}%\".format(accuracy*100))\n","\n","x_tfidf_test = vectorizer.transform(test_df.comment)\n","y_predict = sentiment_fit.predict(x_tfidf_test)\n","test_df['label'] = y_predict\n","test_df[['id', 'label']].to_csv(sample_csv, index=False)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iD6qBLFyYa3l","colab_type":"text"},"cell_type":"markdown","source":["SVM classification"]},{"metadata":{"id":"KAx9vhT-YaGN","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn import svm\n","from sklearn.metrics import accuracy_score\n","model = svm.SVC(kernel='linear')\n","sentiment_fit = model.fit(x_tfidf_train, y_train)\n","print(\"Predict...........\")\n","y_pred = sentiment_fit.predict(x_tfidf_val)\n","accuracy = accuracy_score(y_val, y_pred)\n","print(\"accuracy score: {0:.2f}%\".format(accuracy*100))\n","\n","x_tfidf_test = vectorizer.transform(test_df.comment)\n","y_predict = sentiment_fit.predict(x_tfidf_test)\n","test_df['label'] = y_predict\n","sample_csv = os.path.join(data_path, \"sample_svm.csv\")\n","test_df[['id', 'label']].to_csv(sample_csv, index=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rNCS1pdMYyhE","colab_type":"text"},"cell_type":"markdown","source":["LSTM | CNN model"]},{"metadata":{"id":"ShP8thq6Y0Qv","colab_type":"code","colab":{}},"cell_type":"code","source":["# Keras\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n","from keras.layers.embeddings import Embedding\n","## Plotly\n","import plotly.offline as py\n","import plotly.graph_objs as go\n","py.init_notebook_mode(connected=True)\n","# Others\n","import nltk\n","import string\n","import numpy as np\n","import pandas as pd\n","from nltk.corpus import stopwords\n","\n","from sklearn.manifold import TSNE\n","\n","### Create sequence\n","vocabulary_size = 10000\n","tokenizer = Tokenizer(num_words= vocabulary_size)\n","tokenizer.fit_on_texts(train_df['comment'])\n","train_sequences = tokenizer.texts_to_sequences(train_df['comment'])\n","test_sequences = tokenizer.texts_to_sequences(test_df['comment'])\n","x_train = pad_sequences(train_sequences, maxlen=50)\n","x_test = pad_sequences(test_sequences, maxlen=50)\n","labels = train_df['label']\n","\n","\n","def create_conv_model():\n","    model_conv = Sequential()\n","    model_conv.add(Embedding(vocabulary_size, 100, input_length=50))\n","#     model_conv.add(Dropout(0.2))\n","#     model_conv.add(Conv1D(64, 5, activation='relu'))\n","#     model_conv.add(MaxPooling1D(pool_size=4))\n","    model_conv.add(LSTM(100))\n","    model_conv.add(Dense(1, activation='sigmoid'))\n","    model_conv.compile(loss='binary_crossentropy', optimizer='adam',    metrics=['accuracy'])\n","    return model_conv\n","# model_conv = create_conv_model()\n","# model_conv.fit(x_train, np.array(labels), validation_split=0.4, epochs = 100)\n","\n","# evaluate the model\n","# scores = model.evaluate(X, Y, verbose=0)\n","# print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n"," \n","# serialize model to JSON\n","model_json = model_conv.to_json()\n","with open(\"model.json\", \"w\") as json_file:\n","    json_file.write(model_json)\n","# serialize weights to HDF5\n","model_conv.save_weights(\"model.h5\")\n","print(\"Saved model to disk\")\n"," \n","# later...\n"," \n","from keras.models import model_from_json\n","# load json and create model\n","json_file = open('model.json', 'r')\n","loaded_model_json = json_file.read()\n","json_file.close()\n","loaded_model = model_from_json(loaded_model_json)\n","# load weights into new model\n","loaded_model.load_weights(\"model.h5\")\n","print(\"Loaded model from disk\")\n","\n","\n","y_predict = model_conv.predict_classes(x_test)\n","test_df['label'] = y_predict\n","sample_csv = os.path.join(data_path, \"sample_lstm.csv\")\n","print('sample: ', sample_csv)\n","test_df[['id', 'label']].to_csv(sample_csv, index=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"s6xT7nPjY9Bb","colab_type":"text"},"cell_type":"markdown","source":["Word embedding"]},{"metadata":{"id":"dkDFzHDXY5Bg","colab_type":"code","colab":{}},"cell_type":"code","source":["embeddings_index = dict()\n","with open(os.path.join(data_path, 'glove.6B.100d.txt')) as fo:\n","    for line in fo:\n","        values = line.split()\n","        word = values[0]\n","        coefs = np.asarray(values[1:], dtype='float32')\n","        embeddings_index[word] = coefs\n","    fo.close()\n","         \n","         \n","embedding_matrix = np.zeros((vocabulary_size, 100))\n","for word, index in tokenizer.word_index.items():\n","    if index > vocabulary_size - 1:\n","        break\n","    else:\n","        embedding_vector = embeddings_index.get(word)\n","        if embedding_vector is not None:\n","            embedding_matrix[index] = embedding_vector\n","         \n","## create model\n","model_glove = Sequential()\n","model_glove.add(Embedding(vocabulary_size, 100, input_length=50, weights=[embedding_matrix], trainable=False))\n","model_glove.add(Dropout(0.2))\n","model_glove.add(Conv1D(64, 5, activation='relu'))\n","model_glove.add(MaxPooling1D(pool_size=4))\n","model_glove.add(LSTM(100))\n","model_glove.add(Dense(1, activation='sigmoid'))\n","model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","## Fit train data\n","model_glove.fit(x_train, np.array(labels), validation_split=0.4, epochs = 3)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"p2pK9QeZbOZ7","colab_type":"text"},"cell_type":"markdown","source":["Fasttext"]},{"metadata":{"id":"ry9UvjFrZBub","colab_type":"code","outputId":"0c84237f-8e73-4ce7-bb97-5f2a54042b0e","executionInfo":{"status":"ok","timestamp":1552268167140,"user_tz":-420,"elapsed":25289,"user":{"displayName":"Nghị Nguyễn Đình","photoUrl":"","userId":"02222397772048019559"}},"colab":{"base_uri":"https://localhost:8080/","height":230}},"cell_type":"code","source":["!pip install fasttext"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting fasttext\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/86/ff826211bc9e28d4c371668b30b4b2c38a09127e5e73017b1c0cd52f9dfa/fasttext-0.8.3.tar.gz (73kB)\n","\u001b[K    100% |████████████████████████████████| 81kB 3.2MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1 in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.14.6)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from fasttext) (0.16.0)\n","Building wheels for collected packages: fasttext\n","  Building wheel for fasttext (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/73/8e/5d/ecb50b90adaab5868ae1d8df180f31e55e85c2f055aaf2fb35\n","Successfully built fasttext\n","Installing collected packages: fasttext\n","Successfully installed fasttext-0.8.3\n"],"name":"stdout"}]},{"metadata":{"id":"ZHa7XlfIZCSf","colab_type":"code","colab":{}},"cell_type":"code","source":["import fasttext \n","# CBOW model\n","model = fasttext.cbow('','')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LXEGrUMWchl2","colab_type":"text"},"cell_type":"markdown","source":["CNN-LSTM model"]},{"metadata":{"id":"8jgWl91OZK7T","colab_type":"code","outputId":"d1822200-e56a-429a-ea17-ce07f3368d6e","executionInfo":{"status":"ok","timestamp":1552268197156,"user_tz":-420,"elapsed":5576,"user":{"displayName":"Nghị Nguyễn Đình","photoUrl":"","userId":"02222397772048019559"}},"colab":{"base_uri":"https://localhost:8080/","height":175}},"cell_type":"code","source":["import numpy as np\n","import re\n","import numpy\n","\n","\t\n","word2id= {}\n","id2word={}\n","index = 1\n","maxlen = 0\n","avglen = 0\n","count100 = 0\n","\n","open_files = [train_pos_file, train_neg_file, test_file]\n","\n","save_files = [train_pos_save, train_neg_save, test_save]\n","\n","for open_file, save_file in zip(open_files,save_files):\n","    pos = []\n","    file = open(open_file, 'r')\n","\n","    for aline in file.readlines():\n","        aline = aline.replace('\\n', \"\")\n","        ids = np.array([], dtype='int32')\n","        for word in aline.split(' '):\n","            word = word.lower()\n","            if word in word2id:\n","                ids = np.append(ids, word2id[word])\n","            else:\n","                if word != '':\n","                    # print (word, \"not in vocalbulary\")\n","                    word2id[word] = index\n","                    id2word[index] = word\n","                    ids = np.append(ids, index)\n","                    index = index + 1\n","        if len(ids) > 0:\n","            pos.append(ids)\n","\n","    file.close()\n","    print(('len pos: ', len(pos)))\n","    np.save(save_file, pos)\n","    for li in pos:\n","        if maxlen < len(li):\n","            maxlen = len(li)\n","        avglen += len(li)\n","        if len(li) > 250:\n","            count100+=1\n","    print('avg len: ', avglen / len(pos))\n","\n","print(('word2id: ', len(word2id)))\n","print((\"maxlen\",maxlen))\n","print((\"maxlen250\",count100))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["('len pos: ', 9280)\n","avg len:  18.045150862068965\n","('len pos: ', 6807)\n","avg len:  49.681357426178934\n","('len pos: ', 10981)\n","avg len:  51.766687915490394\n","('word2id: ', 15500)\n","('maxlen', 639)\n","('maxlen250', 30)\n"],"name":"stdout"}]},{"metadata":{"id":"Pilb7D0FZObW","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import os\n","import pickle\n","import math\n","# import cPickle\n","\n","def load_data_shuffle():\n","    train_pos_save = os.path.join(data_path, \"train_pos.npy\")\n","    train_neg_save = os.path.join(data_path, \"train_neg.npy\")\n","    #Load train data\n","    pos_train = np.load(train_pos_save,encoding='ASCII')\n","    neg_train = np.load(train_neg_save,encoding='ASCII')\n","\n","    y_pos_train = []\n","    for i in pos_train:\n","        y_pos_train.append([0,1])\n","    y_pos_train = np.array(y_pos_train)\n","\n","    y_neg_train = []\n","    for i in neg_train:\n","        y_neg_train.append([1, 0])\n","    y_neg_train = np.array(y_neg_train)\n","    \n","#     print('pos train shape: ', pos_train.shape)\n","    print(pos_train)\n","    #split train and validate set\n","    val_len = int(round(len(pos_train)/10))\n","#     print('val_len: ', val_len)\n","\n","    pos_val = pos_train[0:val_len]\n","    pos_train = pos_train[val_len:]\n","    y_pos_val = y_pos_train[0:val_len]\n","    y_pos_train = y_pos_train[val_len:]\n","\n","\n","    neg_val = neg_train[0:val_len]\n","    neg_train = neg_train[val_len:]\n","    y_neg_val = y_neg_train[0:val_len]\n","    y_neg_train = y_neg_train[val_len:]\n","\n","\n","    X_train = np.concatenate([pos_train,neg_train])\n","    y_train = np.concatenate([y_pos_train,y_neg_train])\n","\n","    X_val = np.concatenate([pos_val,neg_val])\n","    y_val = np.concatenate([y_pos_val,y_neg_val])\n","\n","    test_save = os.path.join(data_path, \"test.npy\")\n","    X_test = np.load(test_save, encoding='ASCII')\n","#     X_test = np.concatenate([pos_test,neu_test,neg_test])\n","#     y_test = np.concatenate([y_pos_test,y_neu_test,y_neg_test])\n","\n","    # print X_train.shape, y_train.shape\n","    # print X_test.shape, y_test.shape\n","    # print X_val.shape, y_val.shape\n","\n","    return X_train, y_train, X_val, y_val, X_test\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XAb4GDQBZQnF","colab_type":"code","colab":{}},"cell_type":"code","source":["'''This example demonstrates the use of Convolution1D for text classification.\n","Gets to 0.88 test accuracy after 2 epochs.\n","90s/epoch on Intel i5 2.4Ghz CPU.\n","10s/epoch on Tesla K40 GPU.\n","'''\n","\n","from __future__ import print_function\n","import numpy as np\n","import pickle\n","\n","\n","np.random.seed(1337)  # for reproducibility\n","\n","from keras.preprocessing import sequence\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation, Lambda\n","from keras.layers import Embedding\n","from keras.layers import Convolution1D, LSTM\n","from keras.datasets import imdb\n","from keras import backend as K\n","from keras.optimizers import Adadelta\n","from keras.preprocessing import sequence as sq\n","from keras.layers import Dense, Dropout, Activation, Lambda,merge,Input,TimeDistributed,Flatten, Add\n","from keras.models import Model\n","from keras.callbacks import ModelCheckpoint\n","\n","import keras.backend.tensorflow_backend as K\n","\n","#config = K.tf.ConfigProto(intra_op_parallelism_threads=16, inter_op_parallelism_threads=16, \\\n","#                        allow_soft_placement=True, device_count = {'CPU': 1})\n","\n","\n","# tf_config = K.tf.ConfigProto()\n","# tf_config.gpu_options.allow_growth = True\n","# session = K.tf.Session(config=tf_config)\n","# K.set_session(session)\n","\n","# config = K.tf.ConfigProto(intra_op_parallelism_threads=4, inter_op_parallelism_threads=4, \\\n","#                         allow_soft_placement=True, device_count = {'CPU': 4})\n","# session = K.tf.Session(config=config)\n","# K.set_session(session)\n","\n","# set parameters:\n","max_features = 16000#14300\n","maxlen = 40\n","batch_size = 32\n","embedding_dims = 200\n","nb_filter = 150\n","filter_length = 3\n","hidden_dims = 100\n","nb_epoch = 100\n","\n","accs = []\n","print('Loading data ...', )\n","\n","X_train, y_train, X_val, y_val, X_test = load_data_shuffle()\n","print(len(X_train), 'train sequences')\n","print(len(X_test), 'test sequences')\n","\n","X_train = sq.pad_sequences(X_train, maxlen=maxlen, padding='pre', truncating='post', value=0.0)\n","X_test = sq.pad_sequences(X_test, maxlen=maxlen, padding='pre', truncating='post', value=0.0)\n","X_val = sq.pad_sequences(X_val, maxlen=maxlen, padding='pre', truncating='post', value=0.0)\n","print('X_train shape:', X_train.shape)\n","print('X_val shape:', X_val.shape)\n","print('X_test shape:', X_test.shape)\n","\n","print('Build model...')\n","model = Sequential()\n","\n","input_layer = Input(shape=(maxlen,),dtype='int32', name='main_input')\n","emb_layer = Embedding(max_features,\n","                      embedding_dims,\n","                      input_length=maxlen\n","                      )(input_layer)\n","def max_1d(X):\n","    return K.max(X, axis=1)\n","\n","# we add a Convolution1D, which will learn nb_filter\n","# word group filters of size 3:\n","\n","con3_layer = Convolution1D(nb_filter=nb_filter,\n","                    filter_length=3,\n","                    border_mode='valid',\n","                    activation='relu',\n","                    subsample_length=1)(emb_layer)\n","\n","pool_con3_layer = Lambda(max_1d, output_shape=(nb_filter,))(con3_layer)\n","\n","\n","# we add a Convolution1D, which will learn nb_filter\n","# word group filters of size 4:\n","\n","con4_layer = Convolution1D(nb_filter=nb_filter,\n","                    filter_length=5,\n","                    border_mode='valid',\n","                    activation='relu',\n","                    subsample_length=1)(emb_layer)\n","\n","pool_con4_layer = Lambda(max_1d, output_shape=(nb_filter,))(con4_layer)\n","\n","\n","# we add a Convolution1D, which will learn nb_filter\n","# word group filters of size 5:\n","\n","con5_layer = Convolution1D(nb_filter=nb_filter,\n","                    filter_length=7,\n","                    border_mode='valid',\n","                    activation='relu',\n","                    subsample_length=1)(emb_layer)\n","\n","pool_con5_layer = Lambda(max_1d, output_shape=(nb_filter,))(con5_layer)\n","\n","cnn_layer = Add()([pool_con3_layer, pool_con5_layer,pool_con4_layer])\n","print(pool_con3_layer.shape)\n","print(pool_con5_layer.shape)\n","print(pool_con4_layer.shape)\n","print(cnn_layer.shape)\n","\n","# cnn_layer = merge([pool_con3_layer, pool_con5_layer,pool_con4_layer ], mode='concat')\n","\n","\n","#LSTM\n","\n","\n","x = Embedding(max_features, embedding_dims, input_length=maxlen)(input_layer)\n","lstm_layer = LSTM(150)(x)\n","\n","cnn_lstm_layer = Add()([lstm_layer, cnn_layer])\n","# cnn_lstm_layer = merge([lstm_layer, cnn_layer], mode='concat')\n","\n","dense_layer = Dense(hidden_dims*2, activation='sigmoid')(cnn_lstm_layer)\n","output_layer= Dropout(0.2)(dense_layer)\n","output_layer = Dense(2, trainable=True, activation='sigmoid')(output_layer)\n","\n","\n","\n","\n","model = Model(input=[input_layer], output=[output_layer])\n","adadelta = Adadelta(lr=0.5, rho=0.95, epsilon=1e-06)\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=\"adamax\",\n","              metrics=['accuracy'])\n","checkpoint = ModelCheckpoint(os.path.join(data_path, 'weights.hdf5'),\n","                             monitor='val_acc', verbose=0, save_best_only=True,\n","                             mode='max')\n","model.fit(X_train, y_train,\n","          batch_size=batch_size,\n","          nb_epoch=nb_epoch,\n","          callbacks=[checkpoint],\n","          validation_data=(X_val, y_val))\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UfCY0hdEZU0g","colab_type":"code","colab":{}},"cell_type":"code","source":["model.load_weights(os.path.join(data_path, 'weights.hdf5'))\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=\"adamax\",\n","              metrics=['accuracy'])\n","score, acc = model.evaluate(X_val, y_val, batch_size=batch_size)\n","y_predict = model.predict(X_test, batch_size=batch_size)\n","cls = []\n","print('shape: ', y_predict.shape)\n","print(y_predict[0])\n","print('type: ', type(y_predict[0]))\n","print('shape: ', y_predict[0].shape)\n","for x in y_predict[0]:\n","    print(x)\n","for i in range(len(y_predict)):\n","    if y_predict[i][0] < y_predict[i][1]: \n","        cls.append(0)\n","    else: cls.append(1)\n","print(y_predict)\n","test_df['label'] = np.asarray(cls)\n","sample_csv = os.path.join(data_path, \"sample_cnn-lstm.csv\")\n","test_df[['id', 'label']].to_csv(sample_csv, index=False)\n","\n","print(\"Acc:\", acc)\n","print(\"Score: \", score)\n","# accs.append(acc)\n","# print (\"Average Acc:\", K.np.mean(K.np.array(accs)))"],"execution_count":0,"outputs":[]}]}